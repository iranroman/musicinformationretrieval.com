{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, scipy, matplotlib.pyplot as plt, sklearn, librosa, mir_eval, IPython.display, urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&larr; Back to Index](index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Instrument Classification Using K-Means "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is loosely based on [Lab 3](https://ccrma.stanford.edu/workshops/mir2010/Lab3_2010.pdf) (2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve an audio file, load it into an array, and listen to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[str, int, os.PathLike[Any], sf.SoundFile, audioread.AudioFile, BinaryIO]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmono\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mduration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DTypeLike'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mres_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'soxr_hq'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Tuple[np.ndarray, float]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load an audio file as a floating point time series.\n",
       "\n",
       "Audio will be automatically resampled to the given rate\n",
       "(default ``sr=22050``).\n",
       "\n",
       "To preserve the native sampling rate of the file, use ``sr=None``.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : string, int, pathlib.Path, soundfile.SoundFile, audioread object, or file-like object\n",
       "    path to the input file.\n",
       "\n",
       "    Any codec supported by `soundfile` or `audioread` will work.\n",
       "\n",
       "    Any string file paths, or any object implementing Python's\n",
       "    file interface (e.g. `pathlib.Path`) are supported as `path`.\n",
       "\n",
       "    If the codec is supported by `soundfile`, then `path` can also be\n",
       "    an open file descriptor (int) or an existing `soundfile.SoundFile` object.\n",
       "\n",
       "    Pre-constructed audioread decoders are also supported here, see the example\n",
       "    below.  This can be used, for example, to force a specific decoder rather\n",
       "    than relying upon audioread to select one for you.\n",
       "\n",
       "    .. warning:: audioread support is deprecated as of version 0.10.0.\n",
       "        audioread support be removed in version 1.0.\n",
       "\n",
       "sr : number > 0 [scalar]\n",
       "    target sampling rate\n",
       "\n",
       "    'None' uses the native sampling rate\n",
       "\n",
       "mono : bool\n",
       "    convert signal to mono\n",
       "\n",
       "offset : float\n",
       "    start reading after this time (in seconds)\n",
       "\n",
       "duration : float\n",
       "    only load up to this much audio (in seconds)\n",
       "\n",
       "dtype : numeric type\n",
       "    data type of ``y``\n",
       "\n",
       "res_type : str\n",
       "    resample type (see note)\n",
       "\n",
       "    .. note::\n",
       "        By default, this uses `soxr`'s high-quality mode ('HQ').\n",
       "\n",
       "        For alternative resampling modes, see `resample`\n",
       "\n",
       "    .. note::\n",
       "       `audioread` may truncate the precision of the audio data to 16 bits.\n",
       "\n",
       "       See :ref:`ioformats` for alternate loading methods.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "y : np.ndarray [shape=(n,) or (..., n)]\n",
       "    audio time series. Multi-channel is supported.\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> # Load an ogg vorbis file\n",
       ">>> filename = librosa.ex('trumpet')\n",
       ">>> y, sr = librosa.load(filename)\n",
       ">>> y\n",
       "array([-1.407e-03, -4.461e-04, ..., -3.042e-05,  1.277e-05],\n",
       "      dtype=float32)\n",
       ">>> sr\n",
       "22050\n",
       "\n",
       ">>> # Load a file and resample to 11 KHz\n",
       ">>> filename = librosa.ex('trumpet')\n",
       ">>> y, sr = librosa.load(filename, sr=11025)\n",
       ">>> y\n",
       "array([-8.746e-04, -3.363e-04, ..., -1.301e-05,  0.000e+00],\n",
       "      dtype=float32)\n",
       ">>> sr\n",
       "11025\n",
       "\n",
       ">>> # Load 5 seconds of a file, starting 15 seconds in\n",
       ">>> filename = librosa.ex('brahms')\n",
       ">>> y, sr = librosa.load(filename, offset=15.0, duration=5.0)\n",
       ">>> y\n",
       "array([0.146, 0.144, ..., 0.128, 0.015], dtype=float32)\n",
       ">>> sr\n",
       "22050\n",
       "\n",
       ">>> # Load using an already open SoundFile object\n",
       ">>> import soundfile\n",
       ">>> sfo = soundfile.SoundFile(librosa.ex('brahms'))\n",
       ">>> y, sr = librosa.load(sfo)\n",
       "\n",
       ">>> # Load using an already open audioread object\n",
       ">>> import audioread.ffdec  # Use ffmpeg decoder\n",
       ">>> aro = audioread.ffdec.FFmpegAudioFile(librosa.ex('brahms'))\n",
       ">>> y, sr = librosa.load(aro)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/core/audio.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Onsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect onsets in the audio signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monset_detect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0monset_envelope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhop_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbacktrack\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menergy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0munits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'frames'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Locate note onset events by picking peaks in an onset strength envelope.\n",
       "\n",
       "The `peak_pick` parameters were chosen by large-scale hyper-parameter\n",
       "optimization over the dataset provided by [#]_.\n",
       "\n",
       ".. [#] https://github.com/CPJKU/onset_db\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y : np.ndarray [shape=(n,)]\n",
       "    audio time series, must be monophonic\n",
       "\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "\n",
       "onset_envelope : np.ndarray [shape=(m,)]\n",
       "    (optional) pre-computed onset strength envelope\n",
       "\n",
       "hop_length : int > 0 [scalar]\n",
       "    hop length (in samples)\n",
       "\n",
       "units : {'frames', 'samples', 'time'}\n",
       "    The units to encode detected onset events in.\n",
       "    By default, 'frames' are used.\n",
       "\n",
       "backtrack : bool\n",
       "    If ``True``, detected onset events are backtracked to the nearest\n",
       "    preceding minimum of ``energy``.\n",
       "\n",
       "    This is primarily useful when using onsets as slice points for segmentation.\n",
       "\n",
       "energy : np.ndarray [shape=(m,)] (optional)\n",
       "    An energy function to use for backtracking detected onset events.\n",
       "    If none is provided, then ``onset_envelope`` is used.\n",
       "\n",
       "normalize : bool\n",
       "    If ``True`` (default), normalize the onset envelope to have minimum of 0 and\n",
       "    maximum of 1 prior to detection.  This is helpful for standardizing the\n",
       "    parameters of `librosa.util.peak_pick`.\n",
       "\n",
       "    Otherwise, the onset envelope is left unnormalized.\n",
       "\n",
       "**kwargs : additional keyword arguments\n",
       "    Additional parameters for peak picking.\n",
       "\n",
       "    See `librosa.util.peak_pick` for details.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "onsets : np.ndarray [shape=(n_onsets,)]\n",
       "    estimated positions of detected onsets, in whichever units\n",
       "    are specified.  By default, frame indices.\n",
       "\n",
       "    .. note::\n",
       "        If no onset strength could be detected, onset_detect returns\n",
       "        an empty list.\n",
       "\n",
       "Raises\n",
       "------\n",
       "ParameterError\n",
       "    if neither ``y`` nor ``onsets`` are provided\n",
       "\n",
       "    or if ``units`` is not one of 'frames', 'samples', or 'time'\n",
       "\n",
       "See Also\n",
       "--------\n",
       "onset_strength : compute onset strength per-frame\n",
       "onset_backtrack : backtracking onset events\n",
       "librosa.util.peak_pick : pick peaks from a time series\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Get onset times from a signal\n",
       "\n",
       ">>> y, sr = librosa.load(librosa.ex('trumpet'))\n",
       ">>> librosa.onset.onset_detect(y=y, sr=sr, units='time')\n",
       "array([0.07 , 0.232, 0.395, 0.604, 0.743, 0.929, 1.045, 1.115,\n",
       "       1.416, 1.672, 1.881, 2.043, 2.206, 2.368, 2.554, 3.019])\n",
       "\n",
       "Or use a pre-computed onset envelope\n",
       "\n",
       ">>> o_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
       ">>> times = librosa.times_like(o_env, sr=sr)\n",
       ">>> onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> D = np.abs(librosa.stft(y))\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True)\n",
       ">>> librosa.display.specshow(librosa.amplitude_to_db(D, ref=np.max),\n",
       "...                          x_axis='time', y_axis='log', ax=ax[0])\n",
       ">>> ax[0].set(title='Power spectrogram')\n",
       ">>> ax[0].label_outer()\n",
       ">>> ax[1].plot(times, o_env, label='Onset strength')\n",
       ">>> ax[1].vlines(times[onset_frames], 0, o_env.max(), color='r', alpha=0.9,\n",
       "...            linestyle='--', label='Onsets')\n",
       ">>> ax[1].legend()\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/onset.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.onset.onset_detect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the onsets from units of frames to seconds (and samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_to_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'_ScalarOrSequence[_IntLike_co]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhop_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_fft\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Union[np.floating[Any], np.ndarray]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Convert frame counts to time (seconds).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "frames : np.ndarray [shape=(n,)]\n",
       "    frame index or vector of frame indices\n",
       "sr : number > 0 [scalar]\n",
       "    audio sampling rate\n",
       "hop_length : int > 0 [scalar]\n",
       "    number of samples between successive frames\n",
       "n_fft : None or int > 0 [scalar]\n",
       "    Optional: length of the FFT window.\n",
       "    If given, time conversion will include an offset of ``n_fft // 2``\n",
       "    to counteract windowing effects when using a non-centered STFT.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "times : np.ndarray [shape=(n,)]\n",
       "    time (in seconds) of each given frame number::\n",
       "\n",
       "        times[i] = frames[i] * hop_length / sr\n",
       "\n",
       "See Also\n",
       "--------\n",
       "time_to_frames : convert time values to frame indices\n",
       "frames_to_samples : convert frame indices to sample indices\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> y, sr = librosa.load(librosa.ex('choice'))\n",
       ">>> tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
       ">>> beat_times = librosa.frames_to_time(beats, sr=sr)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/core/convert.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.frames_to_time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_to_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'_ScalarOrSequence[_IntLike_co]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhop_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_fft\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Union[np.integer[Any], np.ndarray]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Convert frame indices to audio sample indices.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "frames : number or np.ndarray [shape=(n,)]\n",
       "    frame index or vector of frame indices\n",
       "hop_length : int > 0 [scalar]\n",
       "    number of samples between successive frames\n",
       "n_fft : None or int > 0 [scalar]\n",
       "    Optional: length of the FFT window.\n",
       "    If given, time conversion will include an offset of ``n_fft // 2``\n",
       "    to counteract windowing effects when using a non-centered STFT.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "times : number or np.ndarray\n",
       "    time (in samples) of each given frame number::\n",
       "\n",
       "        times[i] = frames[i] * hop_length\n",
       "\n",
       "See Also\n",
       "--------\n",
       "frames_to_time : convert frame indices to time values\n",
       "samples_to_frames : convert sample indices to frame indices\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> y, sr = librosa.load(librosa.ex('choice'))\n",
       ">>> tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
       ">>> beat_samples = librosa.frames_to_samples(beats)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/core/convert.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.frames_to_samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to detected onsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmir_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msonify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclicks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a signal with the signal 'click' placed at each specified time\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "times : np.ndarray\n",
       "    times to place clicks, in seconds\n",
       "fs : int\n",
       "    desired sampling rate of the output signal\n",
       "click : np.ndarray\n",
       "    click signal, defaults to a 1 kHz blip\n",
       "length : int\n",
       "    desired number of samples in the output signal,\n",
       "    defaults to ``times.max()*fs + click.shape[0] + 1``\n",
       "\n",
       "Returns\n",
       "-------\n",
       "click_signal : np.ndarray\n",
       "    Synthesized click signal\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/mir_eval/sonify.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mir_eval.sonify.clicks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a set of features from the audio at each onset. Use any of the features we have learned so far: zero crossing rate, spectral moments, MFCCs, chroma, etc. For more, see the [librosa API reference](http://bmcfee.github.io/librosa/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define which features to extract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x, fs):\n",
    "    feature_1 = librosa.zero_crossings(x).sum() # placeholder\n",
    "    feature_2 = 0 # placeholder\n",
    "    return [feature_1, feature_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each onset, extract a feature vector from the signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assumptions:\n",
    "# x: input audio signal\n",
    "# fs: sampling frequency\n",
    "# onset_samples: onsets in units of samples\n",
    "fs = 44100\n",
    "frame_sz = fs*0.100\n",
    "#features = numpy.array([extract_features(x[i:i+frame_sz], fs) for i in onset_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sklearn.preprocessing.MinMaxScaler` to scale your features to be within `[-1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn.preprocessing.MinMaxScaler` not found.\n"
     ]
    }
   ],
   "source": [
    "sklearn.preprocessing.MinMaxScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn.preprocessing.MinMaxScaler.fit_transform` not found.\n"
     ]
    }
   ],
   "source": [
    "sklearn.preprocessing.MinMaxScaler.fit_transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `scatter` to plot features on a 2-D plane. (Choose two features at a time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mplotnonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "x, y : float or array-like, shape (n, )\n",
       "    The data positions.\n",
       "\n",
       "s : float or array-like, shape (n, ), optional\n",
       "    The marker size in points**2 (typographic points are 1/72 in.).\n",
       "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
       "\n",
       "c : array-like or list of colors or color, optional\n",
       "    The marker colors. Possible values:\n",
       "\n",
       "    - A scalar or sequence of n numbers to be mapped to colors using\n",
       "      *cmap* and *norm*.\n",
       "    - A 2D array in which the rows are RGB or RGBA.\n",
       "    - A sequence of colors of length n.\n",
       "    - A single color format string.\n",
       "\n",
       "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
       "    because that is indistinguishable from an array of values to be\n",
       "    colormapped. If you want to specify the same RGB or RGBA value for\n",
       "    all points, use a 2D array with a single row.  Otherwise,\n",
       "    value-matching will have precedence in case of a size matching with\n",
       "    *x* and *y*.\n",
       "\n",
       "    If you wish to specify a single color for all points\n",
       "    prefer the *color* keyword argument.\n",
       "\n",
       "    Defaults to `None`. In that case the marker color is determined\n",
       "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
       "    those are not specified or `None`, the marker color is determined\n",
       "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
       "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
       "\n",
       "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
       "    The marker style. *marker* can be either an instance of the class\n",
       "    or the text shorthand for a particular marker.\n",
       "    See :mod:`matplotlib.markers` for more information about marker\n",
       "    styles.\n",
       "\n",
       "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
       "    The Colormap instance or registered colormap name used to map scalar data\n",
       "    to colors.\n",
       "\n",
       "    This parameter is ignored if *c* is RGB(A).\n",
       "\n",
       "norm : str or `~matplotlib.colors.Normalize`, optional\n",
       "    The normalization method used to scale scalar data to the [0, 1] range\n",
       "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
       "    used, mapping the lowest value to 0 and the highest to 1.\n",
       "\n",
       "    If given, this can be one of the following:\n",
       "\n",
       "    - An instance of `.Normalize` or one of its subclasses\n",
       "      (see :doc:`/tutorials/colors/colormapnorms`).\n",
       "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
       "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
       "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
       "      and instantiated.\n",
       "\n",
       "    This parameter is ignored if *c* is RGB(A).\n",
       "\n",
       "vmin, vmax : float, optional\n",
       "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
       "    the data range that the colormap covers. By default, the colormap covers\n",
       "    the complete value range of the supplied data. It is an error to use\n",
       "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
       "    name together with *vmin*/*vmax* is acceptable).\n",
       "\n",
       "    This parameter is ignored if *c* is RGB(A).\n",
       "\n",
       "alpha : float, default: None\n",
       "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
       "\n",
       "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
       "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
       "    is 'face'. You may want to change this as well.\n",
       "\n",
       "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
       "    The edge color of the marker. Possible values:\n",
       "\n",
       "    - 'face': The edge color will always be the same as the face color.\n",
       "    - 'none': No patch boundary will be drawn.\n",
       "    - A color or sequence of colors.\n",
       "\n",
       "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
       "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
       "    *facecolors*.\n",
       "\n",
       "plotnonfinite : bool, default: False\n",
       "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
       "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
       "    colormap color (see `.Colormap.set_bad`).\n",
       "\n",
       "Returns\n",
       "-------\n",
       "`~matplotlib.collections.PathCollection`\n",
       "\n",
       "Other Parameters\n",
       "----------------\n",
       "data : indexable object, optional\n",
       "    If given, the following parameters also accept a string ``s``, which is\n",
       "    interpreted as ``data[s]`` (unless this raises an exception):\n",
       "\n",
       "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
       "**kwargs : `~matplotlib.collections.Collection` properties\n",
       "\n",
       "See Also\n",
       "--------\n",
       "plot : To plot scatter plots when markers are identical in size and\n",
       "    color.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "* The `.plot` function will be faster for scatterplots where markers\n",
       "  don't vary in size or color.\n",
       "\n",
       "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
       "  case all masks will be combined and only unmasked points will be\n",
       "  plotted.\n",
       "\n",
       "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
       "  may be input as N-D arrays, but within scatter they will be\n",
       "  flattened. The exception is *c*, which will be flattened only if its\n",
       "  size matches the size of *x* and *y*.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/matplotlib/pyplot.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Using K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `KMeans` to cluster your features and compute labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn.cluster.KMeans` not found.\n"
     ]
    }
   ],
   "source": [
    "sklearn.cluster.KMeans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `sklearn.cluster.KMeans.fit_predict` not found.\n"
     ]
    }
   ],
   "source": [
    "sklearn.cluster.KMeans.fit_predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Features by Class Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `scatter`, but this time choose a different marker color (or type) for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mplotnonfinite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "x, y : float or array-like, shape (n, )\n",
       "    The data positions.\n",
       "\n",
       "s : float or array-like, shape (n, ), optional\n",
       "    The marker size in points**2 (typographic points are 1/72 in.).\n",
       "    Default is ``rcParams['lines.markersize'] ** 2``.\n",
       "\n",
       "c : array-like or list of colors or color, optional\n",
       "    The marker colors. Possible values:\n",
       "\n",
       "    - A scalar or sequence of n numbers to be mapped to colors using\n",
       "      *cmap* and *norm*.\n",
       "    - A 2D array in which the rows are RGB or RGBA.\n",
       "    - A sequence of colors of length n.\n",
       "    - A single color format string.\n",
       "\n",
       "    Note that *c* should not be a single numeric RGB or RGBA sequence\n",
       "    because that is indistinguishable from an array of values to be\n",
       "    colormapped. If you want to specify the same RGB or RGBA value for\n",
       "    all points, use a 2D array with a single row.  Otherwise,\n",
       "    value-matching will have precedence in case of a size matching with\n",
       "    *x* and *y*.\n",
       "\n",
       "    If you wish to specify a single color for all points\n",
       "    prefer the *color* keyword argument.\n",
       "\n",
       "    Defaults to `None`. In that case the marker color is determined\n",
       "    by the value of *color*, *facecolor* or *facecolors*. In case\n",
       "    those are not specified or `None`, the marker color is determined\n",
       "    by the next color of the ``Axes``' current \"shape and fill\" color\n",
       "    cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
       "\n",
       "marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
       "    The marker style. *marker* can be either an instance of the class\n",
       "    or the text shorthand for a particular marker.\n",
       "    See :mod:`matplotlib.markers` for more information about marker\n",
       "    styles.\n",
       "\n",
       "cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
       "    The Colormap instance or registered colormap name used to map scalar data\n",
       "    to colors.\n",
       "\n",
       "    This parameter is ignored if *c* is RGB(A).\n",
       "\n",
       "norm : str or `~matplotlib.colors.Normalize`, optional\n",
       "    The normalization method used to scale scalar data to the [0, 1] range\n",
       "    before mapping to colors using *cmap*. By default, a linear scaling is\n",
       "    used, mapping the lowest value to 0 and the highest to 1.\n",
       "\n",
       "    If given, this can be one of the following:\n",
       "\n",
       "    - An instance of `.Normalize` or one of its subclasses\n",
       "      (see :doc:`/tutorials/colors/colormapnorms`).\n",
       "    - A scale name, i.e. one of \"linear\", \"log\", \"symlog\", \"logit\", etc.  For a\n",
       "      list of available scales, call `matplotlib.scale.get_scale_names()`.\n",
       "      In that case, a suitable `.Normalize` subclass is dynamically generated\n",
       "      and instantiated.\n",
       "\n",
       "    This parameter is ignored if *c* is RGB(A).\n",
       "\n",
       "vmin, vmax : float, optional\n",
       "    When using scalar data and no explicit *norm*, *vmin* and *vmax* define\n",
       "    the data range that the colormap covers. By default, the colormap covers\n",
       "    the complete value range of the supplied data. It is an error to use\n",
       "    *vmin*/*vmax* when a *norm* instance is given (but using a `str` *norm*\n",
       "    name together with *vmin*/*vmax* is acceptable).\n",
       "\n",
       "    This parameter is ignored if *c* is RGB(A).\n",
       "\n",
       "alpha : float, default: None\n",
       "    The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
       "\n",
       "linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
       "    The linewidth of the marker edges. Note: The default *edgecolors*\n",
       "    is 'face'. You may want to change this as well.\n",
       "\n",
       "edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
       "    The edge color of the marker. Possible values:\n",
       "\n",
       "    - 'face': The edge color will always be the same as the face color.\n",
       "    - 'none': No patch boundary will be drawn.\n",
       "    - A color or sequence of colors.\n",
       "\n",
       "    For non-filled markers, *edgecolors* is ignored. Instead, the color\n",
       "    is determined like with 'face', i.e. from *c*, *colors*, or\n",
       "    *facecolors*.\n",
       "\n",
       "plotnonfinite : bool, default: False\n",
       "    Whether to plot points with nonfinite *c* (i.e. ``inf``, ``-inf``\n",
       "    or ``nan``). If ``True`` the points are drawn with the *bad*\n",
       "    colormap color (see `.Colormap.set_bad`).\n",
       "\n",
       "Returns\n",
       "-------\n",
       "`~matplotlib.collections.PathCollection`\n",
       "\n",
       "Other Parameters\n",
       "----------------\n",
       "data : indexable object, optional\n",
       "    If given, the following parameters also accept a string ``s``, which is\n",
       "    interpreted as ``data[s]`` (unless this raises an exception):\n",
       "\n",
       "    *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*\n",
       "**kwargs : `~matplotlib.collections.Collection` properties\n",
       "\n",
       "See Also\n",
       "--------\n",
       "plot : To plot scatter plots when markers are identical in size and\n",
       "    color.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "* The `.plot` function will be faster for scatterplots where markers\n",
       "  don't vary in size or color.\n",
       "\n",
       "* Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
       "  case all masks will be combined and only unmasked points will be\n",
       "  plotted.\n",
       "\n",
       "* Fundamentally, scatter works with 1D arrays; *x*, *y*, *s*, and *c*\n",
       "  may be input as N-D arrays, but within scatter they will be\n",
       "  flattened. The exception is *c*, which will be flattened only if its\n",
       "  size matches the size of *x* and *y*.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/matplotlib/pyplot.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to Click Track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a beep for each onset within a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beeps = mir_eval.sonify.clicks(onset_times[labels==0], fs, length=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listen to Clustered Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `concatenated_segments` function from the [feature sonification exercise](feature_sonification.html) to concatenate frames from the same cluster into one signal. Then listen to the signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def concatenate_segments(segments, fs=44100, pad_time=0.300):\n",
    "    padded_segments = [numpy.concatenate([segment, numpy.zeros(int(pad_time*fs))]) for segment in segments]\n",
    "    return numpy.concatenate(padded_segments)\n",
    "##concatenated_signal = concatenate_segments(segments, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare across separate classes. What do you hear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a different number of clusters in `KMeans`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a different initialization method in `KMeans`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different features. Compare tonal features against timbral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        module\n",
       "\u001b[0;31mString form:\u001b[0m <module 'librosa.feature' from '/Users/gijsm/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/feature/__init__.py'>\n",
       "\u001b[0;31mFile:\u001b[0m        ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/feature/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Feature extraction\n",
       "==================\n",
       "\n",
       "Spectral features\n",
       "-----------------\n",
       "\n",
       ".. autosummary::\n",
       "    :toctree: generated/\n",
       "\n",
       "    chroma_stft\n",
       "    chroma_cqt\n",
       "    chroma_cens\n",
       "    chroma_vqt\n",
       "    melspectrogram\n",
       "    mfcc\n",
       "    rms\n",
       "    spectral_centroid\n",
       "    spectral_bandwidth\n",
       "    spectral_contrast\n",
       "    spectral_flatness\n",
       "    spectral_rolloff\n",
       "    poly_features\n",
       "    tonnetz\n",
       "    zero_crossing_rate\n",
       "\n",
       "Rhythm features\n",
       "---------------\n",
       ".. autosummary::\n",
       "    :toctree: generated/\n",
       "\n",
       "    tempo\n",
       "    tempogram\n",
       "    fourier_tempogram\n",
       "    tempogram_ratio\n",
       "\n",
       "Feature manipulation\n",
       "--------------------\n",
       "\n",
       ".. autosummary::\n",
       "    :toctree: generated/\n",
       "\n",
       "    delta\n",
       "    stack_memory\n",
       "\n",
       "\n",
       "Feature inversion\n",
       "-----------------\n",
       "\n",
       ".. autosummary::\n",
       "    :toctree: generated\n",
       "\n",
       "    inverse.mel_to_stft\n",
       "    inverse.mel_to_audio\n",
       "    inverse.mfcc_to_mel\n",
       "    inverse.mfcc_to_audio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = '1_bar_funk_groove.mp3'\n",
    "#filename = '58bpm.wav'\n",
    "#filename = '125_bounce.wav'\n",
    "#filename = 'prelude_cmaj_10s.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&larr; Back to Index](index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
