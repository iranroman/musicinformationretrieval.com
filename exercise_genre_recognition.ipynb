{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "import numpy, scipy, matplotlib.pyplot as plt, sklearn, pandas, librosa, IPython.display, os.path\n",
    "import sklearn.preprocessing\n",
    "import sklearn.svm\n",
    "plt.rcParams['figure.figsize'] = (14, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&larr; Back to Index](index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Genre Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract features from an audio signal.\n",
    "2. Train a genre classifier.\n",
    "3. Use the classifier to classify the genre in a song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download an audio file onto your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_brahms = 'audio/brahms_hungarian_dance_5.mp3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 120 seconds of an audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[str, int, os.PathLike[Any], sf.SoundFile, audioread.AudioFile, BinaryIO]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmono\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mduration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DTypeLike'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mres_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'soxr_hq'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Tuple[np.ndarray, float]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load an audio file as a floating point time series.\n",
       "\n",
       "Audio will be automatically resampled to the given rate\n",
       "(default ``sr=22050``).\n",
       "\n",
       "To preserve the native sampling rate of the file, use ``sr=None``.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : string, int, pathlib.Path, soundfile.SoundFile, audioread object, or file-like object\n",
       "    path to the input file.\n",
       "\n",
       "    Any codec supported by `soundfile` or `audioread` will work.\n",
       "\n",
       "    Any string file paths, or any object implementing Python's\n",
       "    file interface (e.g. `pathlib.Path`) are supported as `path`.\n",
       "\n",
       "    If the codec is supported by `soundfile`, then `path` can also be\n",
       "    an open file descriptor (int) or an existing `soundfile.SoundFile` object.\n",
       "\n",
       "    Pre-constructed audioread decoders are also supported here, see the example\n",
       "    below.  This can be used, for example, to force a specific decoder rather\n",
       "    than relying upon audioread to select one for you.\n",
       "\n",
       "    .. warning:: audioread support is deprecated as of version 0.10.0.\n",
       "        audioread support be removed in version 1.0.\n",
       "\n",
       "sr : number > 0 [scalar]\n",
       "    target sampling rate\n",
       "\n",
       "    'None' uses the native sampling rate\n",
       "\n",
       "mono : bool\n",
       "    convert signal to mono\n",
       "\n",
       "offset : float\n",
       "    start reading after this time (in seconds)\n",
       "\n",
       "duration : float\n",
       "    only load up to this much audio (in seconds)\n",
       "\n",
       "dtype : numeric type\n",
       "    data type of ``y``\n",
       "\n",
       "res_type : str\n",
       "    resample type (see note)\n",
       "\n",
       "    .. note::\n",
       "        By default, this uses `soxr`'s high-quality mode ('HQ').\n",
       "\n",
       "        For alternative resampling modes, see `resample`\n",
       "\n",
       "    .. note::\n",
       "       `audioread` may truncate the precision of the audio data to 16 bits.\n",
       "\n",
       "       See :ref:`ioformats` for alternate loading methods.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "y : np.ndarray [shape=(n,) or (..., n)]\n",
       "    audio time series. Multi-channel is supported.\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> # Load an ogg vorbis file\n",
       ">>> filename = librosa.ex('trumpet')\n",
       ">>> y, sr = librosa.load(filename)\n",
       ">>> y\n",
       "array([-1.407e-03, -4.461e-04, ..., -3.042e-05,  1.277e-05],\n",
       "      dtype=float32)\n",
       ">>> sr\n",
       "22050\n",
       "\n",
       ">>> # Load a file and resample to 11 KHz\n",
       ">>> filename = librosa.ex('trumpet')\n",
       ">>> y, sr = librosa.load(filename, sr=11025)\n",
       ">>> y\n",
       "array([-8.746e-04, -3.363e-04, ..., -1.301e-05,  0.000e+00],\n",
       "      dtype=float32)\n",
       ">>> sr\n",
       "11025\n",
       "\n",
       ">>> # Load 5 seconds of a file, starting 15 seconds in\n",
       ">>> filename = librosa.ex('brahms')\n",
       ">>> y, sr = librosa.load(filename, offset=15.0, duration=5.0)\n",
       ">>> y\n",
       "array([0.146, 0.144, ..., 0.128, 0.015], dtype=float32)\n",
       ">>> sr\n",
       "22050\n",
       "\n",
       ">>> # Load using an already open SoundFile object\n",
       ">>> import soundfile\n",
       ">>> sfo = soundfile.SoundFile(librosa.ex('brahms'))\n",
       ">>> y, sr = librosa.load(sfo)\n",
       "\n",
       ">>> # Load using an already open audioread object\n",
       ">>> import audioread.ffdec  # Use ffmpeg decoder\n",
       ">>> aro = audioread.ffdec.FFmpegAudioFile(librosa.ex('brahms'))\n",
       ">>> y, sr = librosa.load(aro)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/core/audio.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_brahms, fs_brahms = librosa.load(filename_brahms, duration=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time-domain waveform of the audio signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `librosa.display.waveplot` not found.\n"
     ]
    }
   ],
   "source": [
    "librosa.display.waveplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each segment, compute the MFCCs. Experiment with `n_mfcc` to select a different number of coefficients, e.g. 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_mfcc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdct_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ortho'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlifter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Mel-frequency cepstral coefficients (MFCCs)\n",
       "\n",
       ".. warning:: If multi-channel audio input ``y`` is provided, the MFCC\n",
       "    calculation will depend on the peak loudness (in decibels) across\n",
       "    all channels.  The result may differ from independent MFCC calculation\n",
       "    of each channel.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y : np.ndarray [shape=(..., n,)] or None\n",
       "    audio time series. Multi-channel is supported..\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "S : np.ndarray [shape=(..., d, t)] or None\n",
       "    log-power Mel spectrogram\n",
       "n_mfcc : int > 0 [scalar]\n",
       "    number of MFCCs to return\n",
       "dct_type : {1, 2, 3}\n",
       "    Discrete cosine transform (DCT) type.\n",
       "    By default, DCT type-2 is used.\n",
       "norm : None or 'ortho'\n",
       "    If ``dct_type`` is `2 or 3`, setting ``norm='ortho'`` uses an ortho-normal\n",
       "    DCT basis.\n",
       "    Normalization is not supported for ``dct_type=1``.\n",
       "lifter : number >= 0\n",
       "    If ``lifter>0``, apply *liftering* (cepstral filtering) to the MFCCs::\n",
       "        M[n, :] <- M[n, :] * (1 + sin(pi * (n + 1) / lifter) * lifter / 2)\n",
       "    Setting ``lifter >= 2 * n_mfcc`` emphasizes the higher-order coefficients.\n",
       "    As ``lifter`` increases, the coefficient weighting becomes approximately linear.\n",
       "**kwargs : additional keyword arguments to `melspectrogram`\n",
       "    if operating on time series input\n",
       "n_fft : int > 0 [scalar]\n",
       "    length of the FFT window\n",
       "hop_length : int > 0 [scalar]\n",
       "    number of samples between successive frames.\n",
       "    See `librosa.stft`\n",
       "win_length : int <= n_fft [scalar]\n",
       "    Each frame of audio is windowed by `window()`.\n",
       "    The window will be of length `win_length` and then padded\n",
       "    with zeros to match ``n_fft``.\n",
       "    If unspecified, defaults to ``win_length = n_fft``.\n",
       "window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n",
       "    - a window specification (string, tuple, or number);\n",
       "    see `scipy.signal.get_window`\n",
       "    - a window function, such as `scipy.signal.windows.hann`\n",
       "    - a vector or array of length ``n_fft``\n",
       "    .. see also:: `librosa.filters.get_window`\n",
       "center : boolean\n",
       "    - If `True`, the signal ``y`` is padded so that frame\n",
       "    ``t`` is centered at ``y[t * hop_length]``.\n",
       "    - If `False`, then frame ``t`` begins at ``y[t * hop_length]``\n",
       "pad_mode : string\n",
       "    If ``center=True``, the padding mode to use at the edges of the signal.\n",
       "    By default, STFT uses zero padding.\n",
       "power : float > 0 [scalar]\n",
       "    Exponent applied to the spectrum before calculating the melspectrogram when the input is a time signal,\n",
       "    e.g. 1 for magnitude, 2 for power **(default)**, etc.\n",
       "**kwargs : additional keyword arguments for Mel filter bank parameters\n",
       "n_mels : int > 0 [scalar]\n",
       "    number of Mel bands to generate\n",
       "fmin : float >= 0 [scalar]\n",
       "    lowest frequency (in Hz)\n",
       "fmax : float >= 0 [scalar]\n",
       "    highest frequency (in Hz).\n",
       "    If `None`, use ``fmax = sr / 2.0``\n",
       "htk : bool [scalar]\n",
       "    use HTK formula instead of Slaney\n",
       "dtype : np.dtype\n",
       "    The data type of the output basis.\n",
       "    By default, uses 32-bit (single-precision) floating point.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "M : np.ndarray [shape=(..., n_mfcc, t)]\n",
       "    MFCC sequence\n",
       "\n",
       "See Also\n",
       "--------\n",
       "melspectrogram\n",
       "scipy.fftpack.dct\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Generate mfccs from a time series\n",
       "\n",
       ">>> y, sr = librosa.load(librosa.ex('libri1'))\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr)\n",
       "array([[-565.919, -564.288, ..., -426.484, -434.668],\n",
       "       [  10.305,   12.509, ...,   88.43 ,   90.12 ],\n",
       "       ...,\n",
       "       [   2.807,    2.068, ...,   -6.725,   -5.159],\n",
       "       [   2.822,    2.244, ...,   -6.198,   -6.177]], dtype=float32)\n",
       "\n",
       "Using a different hop length and HTK-style Mel frequencies\n",
       "\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr, hop_length=1024, htk=True)\n",
       "array([[-5.471e+02, -5.464e+02, ..., -4.446e+02, -4.200e+02],\n",
       "       [ 1.361e+01,  1.402e+01, ...,  9.764e+01,  9.869e+01],\n",
       "       ...,\n",
       "       [ 4.097e-01, -2.029e+00, ..., -1.051e+01, -1.130e+01],\n",
       "       [-1.119e-01, -1.688e+00, ..., -3.442e+00, -4.687e+00]],\n",
       "      dtype=float32)\n",
       "\n",
       "Use a pre-computed log-power Mel spectrogram\n",
       "\n",
       ">>> S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,\n",
       "...                                    fmax=8000)\n",
       ">>> librosa.feature.mfcc(S=librosa.power_to_db(S))\n",
       "array([[-559.974, -558.449, ..., -411.96 , -420.458],\n",
       "       [  11.018,   13.046, ...,   76.972,   80.888],\n",
       "       ...,\n",
       "       [   2.713,    2.379, ...,    1.464,   -2.835],\n",
       "       [   2.712,    2.619, ...,    2.209,    0.648]], dtype=float32)\n",
       "\n",
       "Get more components\n",
       "\n",
       ">>> mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
       "\n",
       "Visualize the MFCC series\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True)\n",
       ">>> img = librosa.display.specshow(librosa.power_to_db(S, ref=np.max),\n",
       "...                                x_axis='time', y_axis='mel', fmax=8000,\n",
       "...                                ax=ax[0])\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> ax[0].set(title='Mel spectrogram')\n",
       ">>> ax[0].label_outer()\n",
       ">>> img = librosa.display.specshow(mfccs, x_axis='time', ax=ax[1])\n",
       ">>> fig.colorbar(img, ax=[ax[1]])\n",
       ">>> ax[1].set(title='MFCC')\n",
       "\n",
       "Compare different DCT bases\n",
       "\n",
       ">>> m_slaney = librosa.feature.mfcc(y=y, sr=sr, dct_type=2)\n",
       ">>> m_htk = librosa.feature.mfcc(y=y, sr=sr, dct_type=3)\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
       ">>> img1 = librosa.display.specshow(m_slaney, x_axis='time', ax=ax[0])\n",
       ">>> ax[0].set(title='RASTAMAT / Auditory toolbox (dct_type=2)')\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> img2 = librosa.display.specshow(m_htk, x_axis='time', ax=ax[1])\n",
       ">>> ax[1].set(title='HTK-style (dct_type=3)')\n",
       ">>> fig.colorbar(img2, ax=[ax[1]])\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/feature/spectral.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.feature.mfcc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mfcc = 12\n",
    "mfcc_brahms = librosa.feature.mfcc(y=x_brahms, sr=fs_brahms, n_mfcc=n_mfcc).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transpose the result to accommodate scikit-learn which assumes that each row is one observation, and each column is one feature dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5168, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_brahms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the features to have zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_brahms_scaled = scaler.fit_transform(mfcc_brahms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the scaling worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0000000e+00, -5.9051040e-09, -2.3620416e-08, -1.7715312e-08,\n",
       "        0.0000000e+00,  1.1810208e-08,  2.3620416e-08, -5.9051040e-09,\n",
       "        0.0000000e+00,  0.0000000e+00,  5.9051040e-09,  0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_brahms_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_brahms_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b: Repeat steps 1 and 2 for another audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_busta = 'audio/busta_rhymes_hits_for_days.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Download the second audio file in the same manner as the first audio file above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 120 seconds of an audio file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[str, int, os.PathLike[Any], sf.SoundFile, audioread.AudioFile, BinaryIO]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmono\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mduration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'DTypeLike'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mres_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'soxr_hq'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Tuple[np.ndarray, float]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Load an audio file as a floating point time series.\n",
       "\n",
       "Audio will be automatically resampled to the given rate\n",
       "(default ``sr=22050``).\n",
       "\n",
       "To preserve the native sampling rate of the file, use ``sr=None``.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : string, int, pathlib.Path, soundfile.SoundFile, audioread object, or file-like object\n",
       "    path to the input file.\n",
       "\n",
       "    Any codec supported by `soundfile` or `audioread` will work.\n",
       "\n",
       "    Any string file paths, or any object implementing Python's\n",
       "    file interface (e.g. `pathlib.Path`) are supported as `path`.\n",
       "\n",
       "    If the codec is supported by `soundfile`, then `path` can also be\n",
       "    an open file descriptor (int) or an existing `soundfile.SoundFile` object.\n",
       "\n",
       "    Pre-constructed audioread decoders are also supported here, see the example\n",
       "    below.  This can be used, for example, to force a specific decoder rather\n",
       "    than relying upon audioread to select one for you.\n",
       "\n",
       "    .. warning:: audioread support is deprecated as of version 0.10.0.\n",
       "        audioread support be removed in version 1.0.\n",
       "\n",
       "sr : number > 0 [scalar]\n",
       "    target sampling rate\n",
       "\n",
       "    'None' uses the native sampling rate\n",
       "\n",
       "mono : bool\n",
       "    convert signal to mono\n",
       "\n",
       "offset : float\n",
       "    start reading after this time (in seconds)\n",
       "\n",
       "duration : float\n",
       "    only load up to this much audio (in seconds)\n",
       "\n",
       "dtype : numeric type\n",
       "    data type of ``y``\n",
       "\n",
       "res_type : str\n",
       "    resample type (see note)\n",
       "\n",
       "    .. note::\n",
       "        By default, this uses `soxr`'s high-quality mode ('HQ').\n",
       "\n",
       "        For alternative resampling modes, see `resample`\n",
       "\n",
       "    .. note::\n",
       "       `audioread` may truncate the precision of the audio data to 16 bits.\n",
       "\n",
       "       See :ref:`ioformats` for alternate loading methods.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "y : np.ndarray [shape=(n,) or (..., n)]\n",
       "    audio time series. Multi-channel is supported.\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> # Load an ogg vorbis file\n",
       ">>> filename = librosa.ex('trumpet')\n",
       ">>> y, sr = librosa.load(filename)\n",
       ">>> y\n",
       "array([-1.407e-03, -4.461e-04, ..., -3.042e-05,  1.277e-05],\n",
       "      dtype=float32)\n",
       ">>> sr\n",
       "22050\n",
       "\n",
       ">>> # Load a file and resample to 11 KHz\n",
       ">>> filename = librosa.ex('trumpet')\n",
       ">>> y, sr = librosa.load(filename, sr=11025)\n",
       ">>> y\n",
       "array([-8.746e-04, -3.363e-04, ..., -1.301e-05,  0.000e+00],\n",
       "      dtype=float32)\n",
       ">>> sr\n",
       "11025\n",
       "\n",
       ">>> # Load 5 seconds of a file, starting 15 seconds in\n",
       ">>> filename = librosa.ex('brahms')\n",
       ">>> y, sr = librosa.load(filename, offset=15.0, duration=5.0)\n",
       ">>> y\n",
       "array([0.146, 0.144, ..., 0.128, 0.015], dtype=float32)\n",
       ">>> sr\n",
       "22050\n",
       "\n",
       ">>> # Load using an already open SoundFile object\n",
       ">>> import soundfile\n",
       ">>> sfo = soundfile.SoundFile(librosa.ex('brahms'))\n",
       ">>> y, sr = librosa.load(sfo)\n",
       "\n",
       ">>> # Load using an already open audioread object\n",
       ">>> import audioread.ffdec  # Use ffmpeg decoder\n",
       ">>> aro = audioread.ffdec.FFmpegAudioFile(librosa.ex('brahms'))\n",
       ">>> y, sr = librosa.load(aro)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/core/audio.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Load the second audio file in the same manner as the first audio file.\n",
    "# x_busta, fs_busta = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the second audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the time-domain waveform and spectrogram of the second audio file. In what ways does the time-domain waveform look different than the first audio file? What differences in musical attributes might this reflect? What additional insights are gained from plotting the spectrogram? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Plot y versus x as lines and/or markers.\n",
       "\n",
       "Call signatures::\n",
       "\n",
       "    plot([x], y, [fmt], *, data=None, **kwargs)\n",
       "    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
       "\n",
       "The coordinates of the points or line nodes are given by *x*, *y*.\n",
       "\n",
       "The optional parameter *fmt* is a convenient way for defining basic\n",
       "formatting like color, marker and linestyle. It's a shortcut string\n",
       "notation described in the *Notes* section below.\n",
       "\n",
       ">>> plot(x, y)        # plot x and y using default line style and color\n",
       ">>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
       ">>> plot(y)           # plot y using x as index array 0..N-1\n",
       ">>> plot(y, 'r+')     # ditto, but with red plusses\n",
       "\n",
       "You can use `.Line2D` properties as keyword arguments for more\n",
       "control on the appearance. Line properties and *fmt* can be mixed.\n",
       "The following two calls yield identical results:\n",
       "\n",
       ">>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
       ">>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
       "...      linewidth=2, markersize=12)\n",
       "\n",
       "When conflicting with *fmt*, keyword arguments take precedence.\n",
       "\n",
       "\n",
       "**Plotting labelled data**\n",
       "\n",
       "There's a convenient way for plotting objects with labelled data (i.e.\n",
       "data that can be accessed by index ``obj['y']``). Instead of giving\n",
       "the data in *x* and *y*, you can provide the object in the *data*\n",
       "parameter and just give the labels for *x* and *y*::\n",
       "\n",
       ">>> plot('xlabel', 'ylabel', data=obj)\n",
       "\n",
       "All indexable objects are supported. This could e.g. be a `dict`, a\n",
       "`pandas.DataFrame` or a structured numpy array.\n",
       "\n",
       "\n",
       "**Plotting multiple sets of data**\n",
       "\n",
       "There are various ways to plot multiple sets of data.\n",
       "\n",
       "- The most straight forward way is just to call `plot` multiple times.\n",
       "  Example:\n",
       "\n",
       "  >>> plot(x1, y1, 'bo')\n",
       "  >>> plot(x2, y2, 'go')\n",
       "\n",
       "- If *x* and/or *y* are 2D arrays a separate data set will be drawn\n",
       "  for every column. If both *x* and *y* are 2D, they must have the\n",
       "  same shape. If only one of them is 2D with shape (N, m) the other\n",
       "  must have length N and will be used for every data set m.\n",
       "\n",
       "  Example:\n",
       "\n",
       "  >>> x = [1, 2, 3]\n",
       "  >>> y = np.array([[1, 2], [3, 4], [5, 6]])\n",
       "  >>> plot(x, y)\n",
       "\n",
       "  is equivalent to:\n",
       "\n",
       "  >>> for col in range(y.shape[1]):\n",
       "  ...     plot(x, y[:, col])\n",
       "\n",
       "- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
       "  groups::\n",
       "\n",
       "  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
       "\n",
       "  In this case, any additional keyword argument applies to all\n",
       "  datasets. Also, this syntax cannot be combined with the *data*\n",
       "  parameter.\n",
       "\n",
       "By default, each line is assigned a different style specified by a\n",
       "'style cycle'. The *fmt* and line property parameters are only\n",
       "necessary if you want explicit deviations from these defaults.\n",
       "Alternatively, you can also change the style cycle using\n",
       ":rc:`axes.prop_cycle`.\n",
       "\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "x, y : array-like or scalar\n",
       "    The horizontal / vertical coordinates of the data points.\n",
       "    *x* values are optional and default to ``range(len(y))``.\n",
       "\n",
       "    Commonly, these parameters are 1D arrays.\n",
       "\n",
       "    They can also be scalars, or two-dimensional (in that case, the\n",
       "    columns represent separate data sets).\n",
       "\n",
       "    These arguments cannot be passed as keywords.\n",
       "\n",
       "fmt : str, optional\n",
       "    A format string, e.g. 'ro' for red circles. See the *Notes*\n",
       "    section for a full description of the format strings.\n",
       "\n",
       "    Format strings are just an abbreviation for quickly setting\n",
       "    basic line properties. All of these and more can also be\n",
       "    controlled by keyword arguments.\n",
       "\n",
       "    This argument cannot be passed as keyword.\n",
       "\n",
       "data : indexable object, optional\n",
       "    An object with labelled data. If given, provide the label names to\n",
       "    plot in *x* and *y*.\n",
       "\n",
       "    .. note::\n",
       "        Technically there's a slight ambiguity in calls where the\n",
       "        second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n",
       "        could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n",
       "        the former interpretation is chosen, but a warning is issued.\n",
       "        You may suppress the warning by adding an empty format string\n",
       "        ``plot('n', 'o', '', data=obj)``.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "list of `.Line2D`\n",
       "    A list of lines representing the plotted data.\n",
       "\n",
       "Other Parameters\n",
       "----------------\n",
       "scalex, scaley : bool, default: True\n",
       "    These parameters determine if the view limits are adapted to the\n",
       "    data limits. The values are passed on to\n",
       "    `~.axes.Axes.autoscale_view`.\n",
       "\n",
       "**kwargs : `~matplotlib.lines.Line2D` properties, optional\n",
       "    *kwargs* are used to specify properties like a line label (for\n",
       "    auto legends), linewidth, antialiasing, marker face color.\n",
       "    Example::\n",
       "\n",
       "    >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n",
       "    >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n",
       "\n",
       "    If you specify multiple lines with one plot call, the kwargs apply\n",
       "    to all those lines. In case the label object is iterable, each\n",
       "    element is used as labels for each set of data.\n",
       "\n",
       "    Here is a list of available `.Line2D` properties:\n",
       "\n",
       "    Properties:\n",
       "    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n",
       "    alpha: scalar or None\n",
       "    animated: bool\n",
       "    antialiased or aa: bool\n",
       "    clip_box: `.Bbox`\n",
       "    clip_on: bool\n",
       "    clip_path: Patch or (Path, Transform) or None\n",
       "    color or c: color\n",
       "    dash_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
       "    dash_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
       "    dashes: sequence of floats (on/off ink in points) or (None, None)\n",
       "    data: (2, N) array or two 1D arrays\n",
       "    drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
       "    figure: `.Figure`\n",
       "    fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
       "    gapcolor: color or None\n",
       "    gid: str\n",
       "    in_layout: bool\n",
       "    label: object\n",
       "    linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
       "    linewidth or lw: float\n",
       "    marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
       "    markeredgecolor or mec: color\n",
       "    markeredgewidth or mew: float\n",
       "    markerfacecolor or mfc: color\n",
       "    markerfacecoloralt or mfcalt: color\n",
       "    markersize or ms: float\n",
       "    markevery: None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]\n",
       "    mouseover: bool\n",
       "    path_effects: `.AbstractPathEffect`\n",
       "    picker: float or callable[[Artist, Event], tuple[bool, dict]]\n",
       "    pickradius: unknown\n",
       "    rasterized: bool\n",
       "    sketch_params: (scale: float, length: float, randomness: float)\n",
       "    snap: bool or None\n",
       "    solid_capstyle: `.CapStyle` or {'butt', 'projecting', 'round'}\n",
       "    solid_joinstyle: `.JoinStyle` or {'miter', 'round', 'bevel'}\n",
       "    transform: unknown\n",
       "    url: str\n",
       "    visible: bool\n",
       "    xdata: 1D array\n",
       "    ydata: 1D array\n",
       "    zorder: float\n",
       "\n",
       "See Also\n",
       "--------\n",
       "scatter : XY scatter plot with markers of varying size and/or color (\n",
       "    sometimes also called bubble chart).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "**Format Strings**\n",
       "\n",
       "A format string consists of a part for color, marker and line::\n",
       "\n",
       "    fmt = '[marker][line][color]'\n",
       "\n",
       "Each of them is optional. If not provided, the value from the style\n",
       "cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
       "the data will be a line without markers.\n",
       "\n",
       "Other combinations such as ``[color][marker][line]`` are also\n",
       "supported, but note that their parsing may be ambiguous.\n",
       "\n",
       "**Markers**\n",
       "\n",
       "=============   ===============================\n",
       "character       description\n",
       "=============   ===============================\n",
       "``'.'``         point marker\n",
       "``','``         pixel marker\n",
       "``'o'``         circle marker\n",
       "``'v'``         triangle_down marker\n",
       "``'^'``         triangle_up marker\n",
       "``'<'``         triangle_left marker\n",
       "``'>'``         triangle_right marker\n",
       "``'1'``         tri_down marker\n",
       "``'2'``         tri_up marker\n",
       "``'3'``         tri_left marker\n",
       "``'4'``         tri_right marker\n",
       "``'8'``         octagon marker\n",
       "``'s'``         square marker\n",
       "``'p'``         pentagon marker\n",
       "``'P'``         plus (filled) marker\n",
       "``'*'``         star marker\n",
       "``'h'``         hexagon1 marker\n",
       "``'H'``         hexagon2 marker\n",
       "``'+'``         plus marker\n",
       "``'x'``         x marker\n",
       "``'X'``         x (filled) marker\n",
       "``'D'``         diamond marker\n",
       "``'d'``         thin_diamond marker\n",
       "``'|'``         vline marker\n",
       "``'_'``         hline marker\n",
       "=============   ===============================\n",
       "\n",
       "**Line Styles**\n",
       "\n",
       "=============    ===============================\n",
       "character        description\n",
       "=============    ===============================\n",
       "``'-'``          solid line style\n",
       "``'--'``         dashed line style\n",
       "``'-.'``         dash-dot line style\n",
       "``':'``          dotted line style\n",
       "=============    ===============================\n",
       "\n",
       "Example format strings::\n",
       "\n",
       "    'b'    # blue markers with default shape\n",
       "    'or'   # red circles\n",
       "    '-g'   # green solid line\n",
       "    '--'   # dashed line with default color\n",
       "    '^k:'  # black triangle_up markers connected by a dotted line\n",
       "\n",
       "**Colors**\n",
       "\n",
       "The supported color abbreviations are the single letter codes\n",
       "\n",
       "=============    ===============================\n",
       "character        color\n",
       "=============    ===============================\n",
       "``'b'``          blue\n",
       "``'g'``          green\n",
       "``'r'``          red\n",
       "``'c'``          cyan\n",
       "``'m'``          magenta\n",
       "``'y'``          yellow\n",
       "``'k'``          black\n",
       "``'w'``          white\n",
       "=============    ===============================\n",
       "\n",
       "and the ``'CN'`` colors that index into the default property cycle.\n",
       "\n",
       "If the color is the only part of the format string, you can\n",
       "additionally use any  `matplotlib.colors` spec, e.g. full names\n",
       "(``'green'``) or hex strings (``'#008000'``).\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/matplotlib/pyplot.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_fft\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhop_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SupportsArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NestedSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hann'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcenter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpad_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'edge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linear_ramp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reflect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'symmetric'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'empty'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpower\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute a mel-scaled spectrogram.\n",
       "\n",
       "If a spectrogram input ``S`` is provided, then it is mapped directly onto\n",
       "the mel basis by ``mel_f.dot(S)``.\n",
       "\n",
       "If a time-series input ``y, sr`` is provided, then its magnitude spectrogram\n",
       "``S`` is first computed, and then mapped onto the mel scale by\n",
       "``mel_f.dot(S**power)``.\n",
       "\n",
       "By default, ``power=2`` operates on a power spectrum.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y : np.ndarray [shape=(..., n)] or None\n",
       "    audio time-series. Multi-channel is supported.\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "S : np.ndarray [shape=(..., d, t)]\n",
       "    spectrogram\n",
       "n_fft : int > 0 [scalar]\n",
       "    length of the FFT window\n",
       "hop_length : int > 0 [scalar]\n",
       "    number of samples between successive frames.\n",
       "    See `librosa.stft`\n",
       "win_length : int <= n_fft [scalar]\n",
       "    Each frame of audio is windowed by `window()`.\n",
       "    The window will be of length `win_length` and then padded\n",
       "    with zeros to match ``n_fft``.\n",
       "    If unspecified, defaults to ``win_length = n_fft``.\n",
       "window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n",
       "    - a window specification (string, tuple, or number);\n",
       "      see `scipy.signal.get_window`\n",
       "    - a window function, such as `scipy.signal.windows.hann`\n",
       "    - a vector or array of length ``n_fft``\n",
       "    .. see also:: `librosa.filters.get_window`\n",
       "center : boolean\n",
       "    - If `True`, the signal ``y`` is padded so that frame\n",
       "      ``t`` is centered at ``y[t * hop_length]``.\n",
       "    - If `False`, then frame ``t`` begins at ``y[t * hop_length]``\n",
       "pad_mode : string\n",
       "    If ``center=True``, the padding mode to use at the edges of the signal.\n",
       "    By default, STFT uses zero padding.\n",
       "power : float > 0 [scalar]\n",
       "    Exponent for the magnitude melspectrogram.\n",
       "    e.g., 1 for energy, 2 for power, etc.\n",
       "**kwargs : additional keyword arguments for Mel filter bank parameters\n",
       "n_mels : int > 0 [scalar]\n",
       "    number of Mel bands to generate\n",
       "fmin : float >= 0 [scalar]\n",
       "    lowest frequency (in Hz)\n",
       "fmax : float >= 0 [scalar]\n",
       "    highest frequency (in Hz).\n",
       "    If `None`, use ``fmax = sr / 2.0``\n",
       "htk : bool [scalar]\n",
       "    use HTK formula instead of Slaney\n",
       "norm : {None, 'slaney', or number} [scalar]\n",
       "    If 'slaney', divide the triangular mel weights by the width of\n",
       "    the mel band (area normalization).\n",
       "    If numeric, use `librosa.util.normalize` to normalize each filter\n",
       "    by to unit l_p norm. See `librosa.util.normalize` for a full\n",
       "    description of supported norm values (including `+-np.inf`).\n",
       "    Otherwise, leave all the triangles aiming for a peak value of 1.0\n",
       "dtype : np.dtype\n",
       "    The data type of the output basis.\n",
       "    By default, uses 32-bit (single-precision) floating point.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "S : np.ndarray [shape=(..., n_mels, t)]\n",
       "    Mel spectrogram\n",
       "\n",
       "See Also\n",
       "--------\n",
       "librosa.filters.mel : Mel filter bank construction\n",
       "librosa.stft : Short-time Fourier Transform\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> y, sr = librosa.load(librosa.ex('trumpet'))\n",
       ">>> librosa.feature.melspectrogram(y=y, sr=sr)\n",
       "array([[3.837e-06, 1.451e-06, ..., 8.352e-14, 1.296e-11],\n",
       "       [2.213e-05, 7.866e-06, ..., 8.532e-14, 1.329e-11],\n",
       "       ...,\n",
       "       [1.115e-05, 5.192e-06, ..., 3.675e-08, 2.470e-08],\n",
       "       [6.473e-07, 4.402e-07, ..., 1.794e-08, 2.908e-08]],\n",
       "      dtype=float32)\n",
       "\n",
       "Using a pre-computed power spectrogram would give the same result:\n",
       "\n",
       ">>> D = np.abs(librosa.stft(y))**2\n",
       ">>> S = librosa.feature.melspectrogram(S=D, sr=sr)\n",
       "\n",
       "Display of mel-frequency spectrogram coefficients, with custom\n",
       "arguments for mel filterbank construction (default is fmax=sr/2):\n",
       "\n",
       ">>> # Passing through arguments to the Mel filters\n",
       ">>> S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,\n",
       "...                                     fmax=8000)\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> fig, ax = plt.subplots()\n",
       ">>> S_dB = librosa.power_to_db(S, ref=np.max)\n",
       ">>> img = librosa.display.specshow(S_dB, x_axis='time',\n",
       "...                          y_axis='mel', sr=sr,\n",
       "...                          fmax=8000, ax=ax)\n",
       ">>> fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
       ">>> ax.set(title='Mel-frequency spectrogram')\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/feature/spectral.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See http://musicinformationretrieval.com/stft.html for more details on displaying spectrograms.\n",
    "librosa.feature.melspectrogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamplitude_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mamin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_db\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Convert an amplitude spectrogram to dB-scaled spectrogram.\n",
       "\n",
       "This is equivalent to ``power_to_db(S**2, ref=ref**2, amin=amin**2, top_db=top_db)``,\n",
       "but is provided for convenience.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "S : np.ndarray\n",
       "    input amplitude\n",
       "\n",
       "ref : scalar or callable\n",
       "    If scalar, the amplitude ``abs(S)`` is scaled relative to ``ref``:\n",
       "    ``20 * log10(S / ref)``.\n",
       "    Zeros in the output correspond to positions where ``S == ref``.\n",
       "\n",
       "    If callable, the reference value is computed as ``ref(S)``.\n",
       "\n",
       "amin : float > 0 [scalar]\n",
       "    minimum threshold for ``S`` and ``ref``\n",
       "\n",
       "top_db : float >= 0 [scalar]\n",
       "    threshold the output at ``top_db`` below the peak:\n",
       "    ``max(20 * log10(S/ref)) - top_db``\n",
       "\n",
       "Returns\n",
       "-------\n",
       "S_db : np.ndarray\n",
       "    ``S`` measured in dB\n",
       "\n",
       "See Also\n",
       "--------\n",
       "power_to_db, db_to_amplitude\n",
       "\n",
       "Notes\n",
       "-----\n",
       "This function caches at level 30.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/core/spectrum.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.amplitude_to_db?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'np.ndarray'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx_coords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[np.ndarray]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_coords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[np.ndarray]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mx_axis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_axis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhop_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_fft\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfmin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfmax\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[float]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtuning\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'float'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbins_per_octave\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C:maj'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mSa\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[float, int]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmela\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[str, int]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthaat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mauto_aspect\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhtk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0municode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mintervals\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[Union[str, np.ndarray]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0munison\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[str]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0max\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[mplaxes.Axes]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'QuadMesh'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Display a spectrogram/chromagram/cqt/etc.\n",
       "\n",
       "For a detailed overview of this function, see :ref:`sphx_glr_auto_examples_plot_display.py`\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : np.ndarray [shape=(d, n)]\n",
       "    Matrix to display (e.g., spectrogram)\n",
       "\n",
       "sr : number > 0 [scalar]\n",
       "    Sample rate used to determine time scale in x-axis.\n",
       "\n",
       "hop_length : int > 0 [scalar]\n",
       "    Hop length, also used to determine time scale in x-axis\n",
       "\n",
       "n_fft : int > 0 or None\n",
       "    Number of samples per frame in STFT/spectrogram displays.\n",
       "    By default, this will be inferred from the shape of ``data``\n",
       "    as ``2 * (d - 1)``.\n",
       "    If ``data`` was generated using an odd frame length, the correct\n",
       "    value can be specified here.\n",
       "\n",
       "win_length : int > 0 or None\n",
       "    The number of samples per window.\n",
       "    By default, this will be inferred to match ``n_fft``.\n",
       "    This is primarily useful for specifying odd window lengths in\n",
       "    Fourier tempogram displays.\n",
       "\n",
       "x_axis, y_axis : None or str\n",
       "    Range for the x- and y-axes.\n",
       "\n",
       "    Valid types are:\n",
       "\n",
       "    - None, 'none', or 'off' : no axis decoration is displayed.\n",
       "\n",
       "    Frequency types:\n",
       "\n",
       "    - 'linear', 'fft', 'hz' : frequency range is determined by\n",
       "      the FFT window and sampling rate.\n",
       "    - 'log' : the spectrum is displayed on a log scale.\n",
       "    - 'fft_note': the spectrum is displayed on a log scale with pitches marked.\n",
       "    - 'fft_svara': the spectrum is displayed on a log scale with svara marked.\n",
       "    - 'mel' : frequencies are determined by the mel scale.\n",
       "    - 'cqt_hz' : frequencies are determined by the CQT scale.\n",
       "    - 'cqt_note' : pitches are determined by the CQT scale.\n",
       "    - 'cqt_svara' : like `cqt_note` but using Hindustani or Carnatic svara\n",
       "    - 'vqt_fjs' : like `cqt_note` but using Functional Just System (FJS)\n",
       "      notation.  This requires a just intonation-based variable-Q\n",
       "      transform representation.\n",
       "\n",
       "    All frequency types are plotted in units of Hz.\n",
       "\n",
       "    Any spectrogram parameters (hop_length, sr, bins_per_octave, etc.)\n",
       "    used to generate the input data should also be provided when\n",
       "    calling `specshow`.\n",
       "\n",
       "    Categorical types:\n",
       "\n",
       "    - 'chroma' : pitches are determined by the chroma filters.\n",
       "      Pitch classes are arranged at integer locations (0-11) according to\n",
       "      a given key.\n",
       "\n",
       "    - `chroma_h`, `chroma_c`: pitches are determined by chroma filters,\n",
       "      and labeled as svara in the Hindustani (`chroma_h`) or Carnatic (`chroma_c`)\n",
       "      according to a given thaat (Hindustani) or melakarta raga (Carnatic).\n",
       "\n",
       "    - 'chroma_fjs': pitches are determined by chroma filters using just\n",
       "      intonation.  All pitch classes are annotated.\n",
       "\n",
       "    - 'tonnetz' : axes are labeled by Tonnetz dimensions (0-5)\n",
       "    - 'frames' : markers are shown as frame counts.\n",
       "\n",
       "    Time types:\n",
       "\n",
       "    - 'time' : markers are shown as milliseconds, seconds, minutes, or hours.\n",
       "            Values are plotted in units of seconds.\n",
       "    - 'h' : markers are shown as hours, minutes, and seconds.\n",
       "    - 'm' : markers are shown as minutes and seconds.\n",
       "    - 's' : markers are shown as seconds.\n",
       "    - 'ms' : markers are shown as milliseconds.\n",
       "    - 'lag' : like time, but past the halfway point counts as negative values.\n",
       "    - 'lag_h' : same as lag, but in hours, minutes and seconds.\n",
       "    - 'lag_m' : same as lag, but in minutes and seconds.\n",
       "    - 'lag_s' : same as lag, but in seconds.\n",
       "    - 'lag_ms' : same as lag, but in milliseconds.\n",
       "\n",
       "    Rhythm:\n",
       "\n",
       "    - 'tempo' : markers are shown as beats-per-minute (BPM)\n",
       "        using a logarithmic scale.  This is useful for\n",
       "        visualizing the outputs of `feature.tempogram`.\n",
       "\n",
       "    - 'fourier_tempo' : same as `'tempo'`, but used when\n",
       "        tempograms are calculated in the Frequency domain\n",
       "        using `feature.fourier_tempogram`.\n",
       "\n",
       "x_coords, y_coords : np.ndarray [shape=data.shape[0 or 1]]\n",
       "    Optional positioning coordinates of the input data.\n",
       "    These can be use to explicitly set the location of each\n",
       "    element ``data[i, j]``, e.g., for displaying beat-synchronous\n",
       "    features in natural time coordinates.\n",
       "\n",
       "    If not provided, they are inferred from ``x_axis`` and ``y_axis``.\n",
       "\n",
       "fmin : float > 0 [scalar] or None\n",
       "    Frequency of the lowest spectrogram bin.  Used for Mel, CQT, and VQT\n",
       "    scales.\n",
       "\n",
       "    If ``y_axis`` is `cqt_hz` or `cqt_note` and ``fmin`` is not given,\n",
       "    it is set by default to ``note_to_hz('C1')``.\n",
       "\n",
       "fmax : float > 0 [scalar] or None\n",
       "    Used for setting the Mel frequency scales\n",
       "\n",
       "tuning : float\n",
       "    Tuning deviation from A440, in fractions of a bin.\n",
       "\n",
       "    This is used for CQT frequency scales, so that ``fmin`` is adjusted\n",
       "    to ``fmin * 2**(tuning / bins_per_octave)``.\n",
       "\n",
       "bins_per_octave : int > 0 [scalar]\n",
       "    Number of bins per octave.  Used for CQT frequency scale.\n",
       "\n",
       "key : str\n",
       "    The reference key to use when using note axes (`cqt_note`, `chroma`).\n",
       "\n",
       "Sa : float or int\n",
       "    If using Hindustani or Carnatic svara axis decorations, specify Sa.\n",
       "\n",
       "    For `cqt_svara`, ``Sa`` should be specified as a frequency in Hz.\n",
       "\n",
       "    For `chroma_c` or `chroma_h`, ``Sa`` should correspond to the position\n",
       "    of Sa within the chromagram.\n",
       "    If not provided, Sa will default to 0 (equivalent to `C`)\n",
       "\n",
       "mela : str or int, optional\n",
       "    If using `chroma_c` or `cqt_svara` display mode, specify the melakarta raga.\n",
       "\n",
       "thaat : str, optional\n",
       "    If using `chroma_h` display mode, specify the parent thaat.\n",
       "\n",
       "intervals : str or array of floats in [1, 2), optional\n",
       "    If using an FJS notation (`chroma_fjs`, `vqt_fjs`), the interval specification.\n",
       "\n",
       "    See `core.interval_frequencies` for a description of supported values.\n",
       "\n",
       "unison : str, optional\n",
       "    If using an FJS notation (`chroma_fjs`, `vqt_fjs`), the pitch name of the unison\n",
       "    interval.  If not provided, it will be inferred from `fmin` (for VQT display) or\n",
       "    assumed as `'C'` (for chroma display).\n",
       "\n",
       "auto_aspect : bool\n",
       "    Axes will have 'equal' aspect if the horizontal and vertical dimensions\n",
       "    cover the same extent and their types match.\n",
       "\n",
       "    To override, set to `False`.\n",
       "\n",
       "htk : bool\n",
       "    If plotting on a mel frequency axis, specify which version of the mel\n",
       "    scale to use.\n",
       "\n",
       "        - `False`: use Slaney formula (default)\n",
       "        - `True`: use HTK formula\n",
       "\n",
       "    See `core.mel_frequencies` for more information.\n",
       "\n",
       "unicode : bool\n",
       "    If using note or svara decorations, setting `unicode=True`\n",
       "    will use unicode glyphs for accidentals and octave encoding.\n",
       "\n",
       "    Setting `unicode=False` will use ASCII glyphs.  This can be helpful\n",
       "    if your font does not support musical notation symbols.\n",
       "\n",
       "ax : matplotlib.axes.Axes or None\n",
       "    Axes to plot on instead of the default `plt.gca()`.\n",
       "\n",
       "**kwargs : additional keyword arguments\n",
       "    Arguments passed through to `matplotlib.pyplot.pcolormesh`.\n",
       "\n",
       "    By default, the following options are set:\n",
       "\n",
       "        - ``rasterized=True``\n",
       "        - ``shading='auto'``\n",
       "        - ``edgecolors='None'``\n",
       "\n",
       "    The ``cmap`` option if not provided, is inferred from data automatically.\n",
       "    Set ``cmap=None`` to use matplotlib's default colormap.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "colormesh : `matplotlib.collections.QuadMesh`\n",
       "    The color mesh object produced by `matplotlib.pyplot.pcolormesh`\n",
       "\n",
       "See Also\n",
       "--------\n",
       "cmap : Automatic colormap detection\n",
       "matplotlib.pyplot.pcolormesh\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Visualize an STFT power spectrum using default parameters\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> y, sr = librosa.load(librosa.ex('choice'), duration=15)\n",
       ">>> fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
       ">>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
       ">>> img = librosa.display.specshow(D, y_axis='linear', x_axis='time',\n",
       "...                                sr=sr, ax=ax[0])\n",
       ">>> ax[0].set(title='Linear-frequency power spectrogram')\n",
       ">>> ax[0].label_outer()\n",
       "\n",
       "Or on a logarithmic scale, and using a larger hop\n",
       "\n",
       ">>> hop_length = 1024\n",
       ">>> D = librosa.amplitude_to_db(np.abs(librosa.stft(y, hop_length=hop_length)),\n",
       "...                             ref=np.max)\n",
       ">>> librosa.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length,\n",
       "...                          x_axis='time', ax=ax[1])\n",
       ">>> ax[1].set(title='Log-frequency power spectrogram')\n",
       ">>> ax[1].label_outer()\n",
       ">>> fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/display.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.display.specshow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract MFCCs from the second audio file. Be sure to transpose the resulting matrix such that each row is one observation, i.e. one set of MFCCs. Also be sure that the shape and size of the resulting MFCC matrix is equivalent to that for the first audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_mfcc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdct_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ortho'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlifter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Mel-frequency cepstral coefficients (MFCCs)\n",
       "\n",
       ".. warning:: If multi-channel audio input ``y`` is provided, the MFCC\n",
       "    calculation will depend on the peak loudness (in decibels) across\n",
       "    all channels.  The result may differ from independent MFCC calculation\n",
       "    of each channel.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y : np.ndarray [shape=(..., n,)] or None\n",
       "    audio time series. Multi-channel is supported..\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "S : np.ndarray [shape=(..., d, t)] or None\n",
       "    log-power Mel spectrogram\n",
       "n_mfcc : int > 0 [scalar]\n",
       "    number of MFCCs to return\n",
       "dct_type : {1, 2, 3}\n",
       "    Discrete cosine transform (DCT) type.\n",
       "    By default, DCT type-2 is used.\n",
       "norm : None or 'ortho'\n",
       "    If ``dct_type`` is `2 or 3`, setting ``norm='ortho'`` uses an ortho-normal\n",
       "    DCT basis.\n",
       "    Normalization is not supported for ``dct_type=1``.\n",
       "lifter : number >= 0\n",
       "    If ``lifter>0``, apply *liftering* (cepstral filtering) to the MFCCs::\n",
       "        M[n, :] <- M[n, :] * (1 + sin(pi * (n + 1) / lifter) * lifter / 2)\n",
       "    Setting ``lifter >= 2 * n_mfcc`` emphasizes the higher-order coefficients.\n",
       "    As ``lifter`` increases, the coefficient weighting becomes approximately linear.\n",
       "**kwargs : additional keyword arguments to `melspectrogram`\n",
       "    if operating on time series input\n",
       "n_fft : int > 0 [scalar]\n",
       "    length of the FFT window\n",
       "hop_length : int > 0 [scalar]\n",
       "    number of samples between successive frames.\n",
       "    See `librosa.stft`\n",
       "win_length : int <= n_fft [scalar]\n",
       "    Each frame of audio is windowed by `window()`.\n",
       "    The window will be of length `win_length` and then padded\n",
       "    with zeros to match ``n_fft``.\n",
       "    If unspecified, defaults to ``win_length = n_fft``.\n",
       "window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n",
       "    - a window specification (string, tuple, or number);\n",
       "    see `scipy.signal.get_window`\n",
       "    - a window function, such as `scipy.signal.windows.hann`\n",
       "    - a vector or array of length ``n_fft``\n",
       "    .. see also:: `librosa.filters.get_window`\n",
       "center : boolean\n",
       "    - If `True`, the signal ``y`` is padded so that frame\n",
       "    ``t`` is centered at ``y[t * hop_length]``.\n",
       "    - If `False`, then frame ``t`` begins at ``y[t * hop_length]``\n",
       "pad_mode : string\n",
       "    If ``center=True``, the padding mode to use at the edges of the signal.\n",
       "    By default, STFT uses zero padding.\n",
       "power : float > 0 [scalar]\n",
       "    Exponent applied to the spectrum before calculating the melspectrogram when the input is a time signal,\n",
       "    e.g. 1 for magnitude, 2 for power **(default)**, etc.\n",
       "**kwargs : additional keyword arguments for Mel filter bank parameters\n",
       "n_mels : int > 0 [scalar]\n",
       "    number of Mel bands to generate\n",
       "fmin : float >= 0 [scalar]\n",
       "    lowest frequency (in Hz)\n",
       "fmax : float >= 0 [scalar]\n",
       "    highest frequency (in Hz).\n",
       "    If `None`, use ``fmax = sr / 2.0``\n",
       "htk : bool [scalar]\n",
       "    use HTK formula instead of Slaney\n",
       "dtype : np.dtype\n",
       "    The data type of the output basis.\n",
       "    By default, uses 32-bit (single-precision) floating point.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "M : np.ndarray [shape=(..., n_mfcc, t)]\n",
       "    MFCC sequence\n",
       "\n",
       "See Also\n",
       "--------\n",
       "melspectrogram\n",
       "scipy.fftpack.dct\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Generate mfccs from a time series\n",
       "\n",
       ">>> y, sr = librosa.load(librosa.ex('libri1'))\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr)\n",
       "array([[-565.919, -564.288, ..., -426.484, -434.668],\n",
       "       [  10.305,   12.509, ...,   88.43 ,   90.12 ],\n",
       "       ...,\n",
       "       [   2.807,    2.068, ...,   -6.725,   -5.159],\n",
       "       [   2.822,    2.244, ...,   -6.198,   -6.177]], dtype=float32)\n",
       "\n",
       "Using a different hop length and HTK-style Mel frequencies\n",
       "\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr, hop_length=1024, htk=True)\n",
       "array([[-5.471e+02, -5.464e+02, ..., -4.446e+02, -4.200e+02],\n",
       "       [ 1.361e+01,  1.402e+01, ...,  9.764e+01,  9.869e+01],\n",
       "       ...,\n",
       "       [ 4.097e-01, -2.029e+00, ..., -1.051e+01, -1.130e+01],\n",
       "       [-1.119e-01, -1.688e+00, ..., -3.442e+00, -4.687e+00]],\n",
       "      dtype=float32)\n",
       "\n",
       "Use a pre-computed log-power Mel spectrogram\n",
       "\n",
       ">>> S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,\n",
       "...                                    fmax=8000)\n",
       ">>> librosa.feature.mfcc(S=librosa.power_to_db(S))\n",
       "array([[-559.974, -558.449, ..., -411.96 , -420.458],\n",
       "       [  11.018,   13.046, ...,   76.972,   80.888],\n",
       "       ...,\n",
       "       [   2.713,    2.379, ...,    1.464,   -2.835],\n",
       "       [   2.712,    2.619, ...,    2.209,    0.648]], dtype=float32)\n",
       "\n",
       "Get more components\n",
       "\n",
       ">>> mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
       "\n",
       "Visualize the MFCC series\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True)\n",
       ">>> img = librosa.display.specshow(librosa.power_to_db(S, ref=np.max),\n",
       "...                                x_axis='time', y_axis='mel', fmax=8000,\n",
       "...                                ax=ax[0])\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> ax[0].set(title='Mel spectrogram')\n",
       ">>> ax[0].label_outer()\n",
       ">>> img = librosa.display.specshow(mfccs, x_axis='time', ax=ax[1])\n",
       ">>> fig.colorbar(img, ax=[ax[1]])\n",
       ">>> ax[1].set(title='MFCC')\n",
       "\n",
       "Compare different DCT bases\n",
       "\n",
       ">>> m_slaney = librosa.feature.mfcc(y=y, sr=sr, dct_type=2)\n",
       ">>> m_htk = librosa.feature.mfcc(y=y, sr=sr, dct_type=3)\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
       ">>> img1 = librosa.display.specshow(m_slaney, x_axis='time', ax=ax[0])\n",
       ">>> ax[0].set(title='RASTAMAT / Auditory toolbox (dct_type=2)')\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> img2 = librosa.display.specshow(m_htk, x_axis='time', ax=ax[1])\n",
       ">>> ax[1].set(title='HTK-style (dct_type=3)')\n",
       ">>> fig.colorbar(img2, ax=[ax[1]])\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/feature/spectral.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.feature.mfcc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# mfcc_busta ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mfcc_busta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the resulting MFCC features to have approximately zero mean and unit variance. Re-use the scaler from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Perform standardization by centering and scaling.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
       "    The data used to scale along the features axis.\n",
       "copy : bool, default=None\n",
       "    Copy the input X or not.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
       "    Transformed array.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler.transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# mfcc_busta_scaled ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the mean of the MFCCs for the second audio file is approximately equal to zero and the variance is approximately equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `mfcc_busta_scaled.mean` not found.\n"
     ]
    }
   ],
   "source": [
    "mfcc_busta_scaled.mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `mfcc_busta_scaled.std` not found.\n"
     ]
    }
   ],
   "source": [
    "mfcc_busta_scaled.std?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all of the scaled feature vectors into one feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = numpy.vstack((mfcc_brahms_scaled, mfcc_busta_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a vector of ground-truth labels, where 0 refers to the first audio file, and 1 refers to the second audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = numpy.concatenate((numpy.zeros(len(mfcc_brahms_scaled)), numpy.ones(len(mfcc_busta_scaled))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a classifer model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "model = sklearn.svm.SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Fit the SVM model according to the given training data.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
       "    Training vectors, where `n_samples` is the number of samples\n",
       "    and `n_features` is the number of features.\n",
       "    For kernel=\"precomputed\", the expected shape of X is\n",
       "    (n_samples, n_samples).\n",
       "\n",
       "y : array-like of shape (n_samples,)\n",
       "    Target values (class labels in classification, real numbers in\n",
       "    regression).\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Per-sample weights. Rescale C per sample. Higher weights\n",
       "    force the classifier to put more emphasis on these points.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "self : object\n",
       "    Fitted estimator.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
       "X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
       "\n",
       "If X is a dense array, then the other methods will not support sparse\n",
       "matrices as input.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/sklearn/svm/_base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the classifier, we will extract an unused 10-second segment from the earlier audio fields as test excerpts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_brahms_test, fs_brahms = librosa.load(filename_brahms, duration=10, offset=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1773] error: part2_3_length (736) too large for available bit count (728)\n"
     ]
    }
   ],
   "source": [
    "x_busta_test, fs_busta = librosa.load(filename_busta, duration=10, offset=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to both of the test audio excerpts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mautoplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0melement_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Create an audio object.\n",
       "\n",
       "When this object is returned by an input cell or passed to the\n",
       "display function, it will result in Audio controls being displayed\n",
       "in the frontend (only works in the notebook).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : numpy array, list, unicode, str or bytes\n",
       "    Can be one of\n",
       "\n",
       "      * Numpy 1d array containing the desired waveform (mono)\n",
       "      * Numpy 2d array containing waveforms for each channel.\n",
       "        Shape=(NCHAN, NSAMPLES). For the standard channel order, see\n",
       "        http://msdn.microsoft.com/en-us/library/windows/hardware/dn653308(v=vs.85).aspx\n",
       "      * List of float or integer representing the waveform (mono)\n",
       "      * String containing the filename\n",
       "      * Bytestring containing raw PCM data or\n",
       "      * URL pointing to a file on the web.\n",
       "\n",
       "    If the array option is used, the waveform will be normalized.\n",
       "\n",
       "    If a filename or url is used, the format support will be browser\n",
       "    dependent.\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "embed : boolean\n",
       "    Should the audio data be embedded using a data URI (True) or should\n",
       "    the original source be referenced. Set this to True if you want the\n",
       "    audio to playable later with no internet connection in the notebook.\n",
       "\n",
       "    Default is `True`, unless the keyword argument `url` is set, then\n",
       "    default value is `False`.\n",
       "rate : integer\n",
       "    The sampling rate of the raw data.\n",
       "    Only required when data parameter is being used as an array\n",
       "autoplay : bool\n",
       "    Set to True if the audio should immediately start playing.\n",
       "    Default is `False`.\n",
       "normalize : bool\n",
       "    Whether audio should be normalized (rescaled) to the maximum possible\n",
       "    range. Default is `True`. When set to `False`, `data` must be between\n",
       "    -1 and 1 (inclusive), otherwise an error is raised.\n",
       "    Applies only when `data` is a list or array of samples; other types of\n",
       "    audio are never normalized.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> import pytest\n",
       ">>> np = pytest.importorskip(\"numpy\")\n",
       "\n",
       "Generate a sound\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> framerate = 44100\n",
       ">>> t = np.linspace(0,5,framerate*5)\n",
       ">>> data = np.sin(2*np.pi*220*t) + np.sin(2*np.pi*224*t)\n",
       ">>> Audio(data, rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "Can also do stereo or more channels\n",
       "\n",
       ">>> dataleft = np.sin(2*np.pi*220*t)\n",
       ">>> dataright = np.sin(2*np.pi*224*t)\n",
       ">>> Audio([dataleft, dataright], rate=framerate)\n",
       "<IPython.lib.display.Audio object>\n",
       "\n",
       "From URL:\n",
       "\n",
       ">>> Audio(\"http://www.nch.com.au/acm/8k16bitpcm.wav\")  # doctest: +SKIP\n",
       ">>> Audio(url=\"http://www.w3schools.com/html/horse.ogg\")  # doctest: +SKIP\n",
       "\n",
       "From a File:\n",
       "\n",
       ">>> Audio('IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       ">>> Audio(filename='IPython/lib/tests/test.wav')  # doctest: +SKIP\n",
       "\n",
       "From Bytes:\n",
       "\n",
       ">>> Audio(b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       ">>> Audio(data=b'RAW_WAV_DATA..')  # doctest: +SKIP\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ipywidgets.Audio\n",
       "\n",
       "     Audio widget with more more flexibility and options.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a display object given raw data.\n",
       "\n",
       "When this object is returned by an expression or passed to the\n",
       "display function, it will result in the data being displayed\n",
       "in the frontend. The MIME type of the data should match the\n",
       "subclasses used, so the Png subclass should be used for 'image/png'\n",
       "data. If the data is a URL, the data will first be downloaded\n",
       "and then displayed.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : unicode, str or bytes\n",
       "    The raw data or a URL or file to load the data from\n",
       "url : unicode\n",
       "    A URL to download the data from.\n",
       "filename : unicode\n",
       "    Path to a local file to load the data from.\n",
       "metadata : dict\n",
       "    Dict of metadata associated to be the object when displayed\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/IPython/lib/display.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPython.display.Audio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute MFCCs from both of the test audio excerpts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_mfcc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdct_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ortho'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlifter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Mel-frequency cepstral coefficients (MFCCs)\n",
       "\n",
       ".. warning:: If multi-channel audio input ``y`` is provided, the MFCC\n",
       "    calculation will depend on the peak loudness (in decibels) across\n",
       "    all channels.  The result may differ from independent MFCC calculation\n",
       "    of each channel.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y : np.ndarray [shape=(..., n,)] or None\n",
       "    audio time series. Multi-channel is supported..\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "S : np.ndarray [shape=(..., d, t)] or None\n",
       "    log-power Mel spectrogram\n",
       "n_mfcc : int > 0 [scalar]\n",
       "    number of MFCCs to return\n",
       "dct_type : {1, 2, 3}\n",
       "    Discrete cosine transform (DCT) type.\n",
       "    By default, DCT type-2 is used.\n",
       "norm : None or 'ortho'\n",
       "    If ``dct_type`` is `2 or 3`, setting ``norm='ortho'`` uses an ortho-normal\n",
       "    DCT basis.\n",
       "    Normalization is not supported for ``dct_type=1``.\n",
       "lifter : number >= 0\n",
       "    If ``lifter>0``, apply *liftering* (cepstral filtering) to the MFCCs::\n",
       "        M[n, :] <- M[n, :] * (1 + sin(pi * (n + 1) / lifter) * lifter / 2)\n",
       "    Setting ``lifter >= 2 * n_mfcc`` emphasizes the higher-order coefficients.\n",
       "    As ``lifter`` increases, the coefficient weighting becomes approximately linear.\n",
       "**kwargs : additional keyword arguments to `melspectrogram`\n",
       "    if operating on time series input\n",
       "n_fft : int > 0 [scalar]\n",
       "    length of the FFT window\n",
       "hop_length : int > 0 [scalar]\n",
       "    number of samples between successive frames.\n",
       "    See `librosa.stft`\n",
       "win_length : int <= n_fft [scalar]\n",
       "    Each frame of audio is windowed by `window()`.\n",
       "    The window will be of length `win_length` and then padded\n",
       "    with zeros to match ``n_fft``.\n",
       "    If unspecified, defaults to ``win_length = n_fft``.\n",
       "window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n",
       "    - a window specification (string, tuple, or number);\n",
       "    see `scipy.signal.get_window`\n",
       "    - a window function, such as `scipy.signal.windows.hann`\n",
       "    - a vector or array of length ``n_fft``\n",
       "    .. see also:: `librosa.filters.get_window`\n",
       "center : boolean\n",
       "    - If `True`, the signal ``y`` is padded so that frame\n",
       "    ``t`` is centered at ``y[t * hop_length]``.\n",
       "    - If `False`, then frame ``t`` begins at ``y[t * hop_length]``\n",
       "pad_mode : string\n",
       "    If ``center=True``, the padding mode to use at the edges of the signal.\n",
       "    By default, STFT uses zero padding.\n",
       "power : float > 0 [scalar]\n",
       "    Exponent applied to the spectrum before calculating the melspectrogram when the input is a time signal,\n",
       "    e.g. 1 for magnitude, 2 for power **(default)**, etc.\n",
       "**kwargs : additional keyword arguments for Mel filter bank parameters\n",
       "n_mels : int > 0 [scalar]\n",
       "    number of Mel bands to generate\n",
       "fmin : float >= 0 [scalar]\n",
       "    lowest frequency (in Hz)\n",
       "fmax : float >= 0 [scalar]\n",
       "    highest frequency (in Hz).\n",
       "    If `None`, use ``fmax = sr / 2.0``\n",
       "htk : bool [scalar]\n",
       "    use HTK formula instead of Slaney\n",
       "dtype : np.dtype\n",
       "    The data type of the output basis.\n",
       "    By default, uses 32-bit (single-precision) floating point.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "M : np.ndarray [shape=(..., n_mfcc, t)]\n",
       "    MFCC sequence\n",
       "\n",
       "See Also\n",
       "--------\n",
       "melspectrogram\n",
       "scipy.fftpack.dct\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Generate mfccs from a time series\n",
       "\n",
       ">>> y, sr = librosa.load(librosa.ex('libri1'))\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr)\n",
       "array([[-565.919, -564.288, ..., -426.484, -434.668],\n",
       "       [  10.305,   12.509, ...,   88.43 ,   90.12 ],\n",
       "       ...,\n",
       "       [   2.807,    2.068, ...,   -6.725,   -5.159],\n",
       "       [   2.822,    2.244, ...,   -6.198,   -6.177]], dtype=float32)\n",
       "\n",
       "Using a different hop length and HTK-style Mel frequencies\n",
       "\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr, hop_length=1024, htk=True)\n",
       "array([[-5.471e+02, -5.464e+02, ..., -4.446e+02, -4.200e+02],\n",
       "       [ 1.361e+01,  1.402e+01, ...,  9.764e+01,  9.869e+01],\n",
       "       ...,\n",
       "       [ 4.097e-01, -2.029e+00, ..., -1.051e+01, -1.130e+01],\n",
       "       [-1.119e-01, -1.688e+00, ..., -3.442e+00, -4.687e+00]],\n",
       "      dtype=float32)\n",
       "\n",
       "Use a pre-computed log-power Mel spectrogram\n",
       "\n",
       ">>> S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,\n",
       "...                                    fmax=8000)\n",
       ">>> librosa.feature.mfcc(S=librosa.power_to_db(S))\n",
       "array([[-559.974, -558.449, ..., -411.96 , -420.458],\n",
       "       [  11.018,   13.046, ...,   76.972,   80.888],\n",
       "       ...,\n",
       "       [   2.713,    2.379, ...,    1.464,   -2.835],\n",
       "       [   2.712,    2.619, ...,    2.209,    0.648]], dtype=float32)\n",
       "\n",
       "Get more components\n",
       "\n",
       ">>> mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
       "\n",
       "Visualize the MFCC series\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True)\n",
       ">>> img = librosa.display.specshow(librosa.power_to_db(S, ref=np.max),\n",
       "...                                x_axis='time', y_axis='mel', fmax=8000,\n",
       "...                                ax=ax[0])\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> ax[0].set(title='Mel spectrogram')\n",
       ">>> ax[0].label_outer()\n",
       ">>> img = librosa.display.specshow(mfccs, x_axis='time', ax=ax[1])\n",
       ">>> fig.colorbar(img, ax=[ax[1]])\n",
       ">>> ax[1].set(title='MFCC')\n",
       "\n",
       "Compare different DCT bases\n",
       "\n",
       ">>> m_slaney = librosa.feature.mfcc(y=y, sr=sr, dct_type=2)\n",
       ">>> m_htk = librosa.feature.mfcc(y=y, sr=sr, dct_type=3)\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
       ">>> img1 = librosa.display.specshow(m_slaney, x_axis='time', ax=ax[0])\n",
       ">>> ax[0].set(title='RASTAMAT / Auditory toolbox (dct_type=2)')\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> img2 = librosa.display.specshow(m_htk, x_axis='time', ax=ax[1])\n",
       ">>> ax[1].set(title='HTK-style (dct_type=3)')\n",
       ">>> fig.colorbar(img2, ax=[ax[1]])\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/feature/spectral.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.feature.mfcc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22050\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_mfcc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdct_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ortho'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlifter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Mel-frequency cepstral coefficients (MFCCs)\n",
       "\n",
       ".. warning:: If multi-channel audio input ``y`` is provided, the MFCC\n",
       "    calculation will depend on the peak loudness (in decibels) across\n",
       "    all channels.  The result may differ from independent MFCC calculation\n",
       "    of each channel.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y : np.ndarray [shape=(..., n,)] or None\n",
       "    audio time series. Multi-channel is supported..\n",
       "sr : number > 0 [scalar]\n",
       "    sampling rate of ``y``\n",
       "S : np.ndarray [shape=(..., d, t)] or None\n",
       "    log-power Mel spectrogram\n",
       "n_mfcc : int > 0 [scalar]\n",
       "    number of MFCCs to return\n",
       "dct_type : {1, 2, 3}\n",
       "    Discrete cosine transform (DCT) type.\n",
       "    By default, DCT type-2 is used.\n",
       "norm : None or 'ortho'\n",
       "    If ``dct_type`` is `2 or 3`, setting ``norm='ortho'`` uses an ortho-normal\n",
       "    DCT basis.\n",
       "    Normalization is not supported for ``dct_type=1``.\n",
       "lifter : number >= 0\n",
       "    If ``lifter>0``, apply *liftering* (cepstral filtering) to the MFCCs::\n",
       "        M[n, :] <- M[n, :] * (1 + sin(pi * (n + 1) / lifter) * lifter / 2)\n",
       "    Setting ``lifter >= 2 * n_mfcc`` emphasizes the higher-order coefficients.\n",
       "    As ``lifter`` increases, the coefficient weighting becomes approximately linear.\n",
       "**kwargs : additional keyword arguments to `melspectrogram`\n",
       "    if operating on time series input\n",
       "n_fft : int > 0 [scalar]\n",
       "    length of the FFT window\n",
       "hop_length : int > 0 [scalar]\n",
       "    number of samples between successive frames.\n",
       "    See `librosa.stft`\n",
       "win_length : int <= n_fft [scalar]\n",
       "    Each frame of audio is windowed by `window()`.\n",
       "    The window will be of length `win_length` and then padded\n",
       "    with zeros to match ``n_fft``.\n",
       "    If unspecified, defaults to ``win_length = n_fft``.\n",
       "window : string, tuple, number, function, or np.ndarray [shape=(n_fft,)]\n",
       "    - a window specification (string, tuple, or number);\n",
       "    see `scipy.signal.get_window`\n",
       "    - a window function, such as `scipy.signal.windows.hann`\n",
       "    - a vector or array of length ``n_fft``\n",
       "    .. see also:: `librosa.filters.get_window`\n",
       "center : boolean\n",
       "    - If `True`, the signal ``y`` is padded so that frame\n",
       "    ``t`` is centered at ``y[t * hop_length]``.\n",
       "    - If `False`, then frame ``t`` begins at ``y[t * hop_length]``\n",
       "pad_mode : string\n",
       "    If ``center=True``, the padding mode to use at the edges of the signal.\n",
       "    By default, STFT uses zero padding.\n",
       "power : float > 0 [scalar]\n",
       "    Exponent applied to the spectrum before calculating the melspectrogram when the input is a time signal,\n",
       "    e.g. 1 for magnitude, 2 for power **(default)**, etc.\n",
       "**kwargs : additional keyword arguments for Mel filter bank parameters\n",
       "n_mels : int > 0 [scalar]\n",
       "    number of Mel bands to generate\n",
       "fmin : float >= 0 [scalar]\n",
       "    lowest frequency (in Hz)\n",
       "fmax : float >= 0 [scalar]\n",
       "    highest frequency (in Hz).\n",
       "    If `None`, use ``fmax = sr / 2.0``\n",
       "htk : bool [scalar]\n",
       "    use HTK formula instead of Slaney\n",
       "dtype : np.dtype\n",
       "    The data type of the output basis.\n",
       "    By default, uses 32-bit (single-precision) floating point.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "M : np.ndarray [shape=(..., n_mfcc, t)]\n",
       "    MFCC sequence\n",
       "\n",
       "See Also\n",
       "--------\n",
       "melspectrogram\n",
       "scipy.fftpack.dct\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Generate mfccs from a time series\n",
       "\n",
       ">>> y, sr = librosa.load(librosa.ex('libri1'))\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr)\n",
       "array([[-565.919, -564.288, ..., -426.484, -434.668],\n",
       "       [  10.305,   12.509, ...,   88.43 ,   90.12 ],\n",
       "       ...,\n",
       "       [   2.807,    2.068, ...,   -6.725,   -5.159],\n",
       "       [   2.822,    2.244, ...,   -6.198,   -6.177]], dtype=float32)\n",
       "\n",
       "Using a different hop length and HTK-style Mel frequencies\n",
       "\n",
       ">>> librosa.feature.mfcc(y=y, sr=sr, hop_length=1024, htk=True)\n",
       "array([[-5.471e+02, -5.464e+02, ..., -4.446e+02, -4.200e+02],\n",
       "       [ 1.361e+01,  1.402e+01, ...,  9.764e+01,  9.869e+01],\n",
       "       ...,\n",
       "       [ 4.097e-01, -2.029e+00, ..., -1.051e+01, -1.130e+01],\n",
       "       [-1.119e-01, -1.688e+00, ..., -3.442e+00, -4.687e+00]],\n",
       "      dtype=float32)\n",
       "\n",
       "Use a pre-computed log-power Mel spectrogram\n",
       "\n",
       ">>> S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,\n",
       "...                                    fmax=8000)\n",
       ">>> librosa.feature.mfcc(S=librosa.power_to_db(S))\n",
       "array([[-559.974, -558.449, ..., -411.96 , -420.458],\n",
       "       [  11.018,   13.046, ...,   76.972,   80.888],\n",
       "       ...,\n",
       "       [   2.713,    2.379, ...,    1.464,   -2.835],\n",
       "       [   2.712,    2.619, ...,    2.209,    0.648]], dtype=float32)\n",
       "\n",
       "Get more components\n",
       "\n",
       ">>> mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
       "\n",
       "Visualize the MFCC series\n",
       "\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True)\n",
       ">>> img = librosa.display.specshow(librosa.power_to_db(S, ref=np.max),\n",
       "...                                x_axis='time', y_axis='mel', fmax=8000,\n",
       "...                                ax=ax[0])\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> ax[0].set(title='Mel spectrogram')\n",
       ">>> ax[0].label_outer()\n",
       ">>> img = librosa.display.specshow(mfccs, x_axis='time', ax=ax[1])\n",
       ">>> fig.colorbar(img, ax=[ax[1]])\n",
       ">>> ax[1].set(title='MFCC')\n",
       "\n",
       "Compare different DCT bases\n",
       "\n",
       ">>> m_slaney = librosa.feature.mfcc(y=y, sr=sr, dct_type=2)\n",
       ">>> m_htk = librosa.feature.mfcc(y=y, sr=sr, dct_type=3)\n",
       ">>> fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
       ">>> img1 = librosa.display.specshow(m_slaney, x_axis='time', ax=ax[0])\n",
       ">>> ax[0].set(title='RASTAMAT / Auditory toolbox (dct_type=2)')\n",
       ">>> fig.colorbar(img, ax=[ax[0]])\n",
       ">>> img2 = librosa.display.specshow(m_htk, x_axis='time', ax=ax[1])\n",
       ">>> ax[1].set(title='HTK-style (dct_type=3)')\n",
       ">>> fig.colorbar(img2, ax=[ax[1]])\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/librosa/feature/spectral.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "librosa.feature.mfcc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the MFCCs using the previous scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Perform standardization by centering and scaling.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
       "    The data used to scale along the features axis.\n",
       "copy : bool, default=None\n",
       "    Copy the input X or not.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
       "    Transformed array.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler.transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Perform standardization by centering and scaling.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : {array-like, sparse matrix of shape (n_samples, n_features)\n",
       "    The data used to scale along the features axis.\n",
       "copy : bool, default=None\n",
       "    Copy the input X or not.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
       "    Transformed array.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scaler.transform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all test features together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m       \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same_kind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mCall signature:\u001b[0m  \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m            _ArrayFunctionDispatcher\n",
       "\u001b[0;31mString form:\u001b[0m     <function vstack at 0x1088fa480>\n",
       "\u001b[0;31mFile:\u001b[0m            ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/numpy/core/shape_base.py\n",
       "\u001b[0;31mDocstring:\u001b[0m      \n",
       "Stack arrays in sequence vertically (row wise).\n",
       "\n",
       "This is equivalent to concatenation along the first axis after 1-D arrays\n",
       "of shape `(N,)` have been reshaped to `(1,N)`. Rebuilds arrays divided by\n",
       "`vsplit`.\n",
       "\n",
       "This function makes most sense for arrays with up to 3 dimensions. For\n",
       "instance, for pixel-data with a height (first axis), width (second axis),\n",
       "and r/g/b channels (third axis). The functions `concatenate`, `stack` and\n",
       "`block` provide more general stacking and concatenation operations.\n",
       "\n",
       "``np.row_stack`` is an alias for `vstack`. They are the same function.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "tup : sequence of ndarrays\n",
       "    The arrays must have the same shape along all but the first axis.\n",
       "    1-D arrays must have the same length.\n",
       "\n",
       "dtype : str or dtype\n",
       "    If provided, the destination array will have this dtype. Cannot be\n",
       "    provided together with `out`.\n",
       "\n",
       ".. versionadded:: 1.24\n",
       "\n",
       "casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
       "    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n",
       "\n",
       ".. versionadded:: 1.24\n",
       "\n",
       "Returns\n",
       "-------\n",
       "stacked : ndarray\n",
       "    The array formed by stacking the given arrays, will be at least 2-D.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "concatenate : Join a sequence of arrays along an existing axis.\n",
       "stack : Join a sequence of arrays along a new axis.\n",
       "block : Assemble an nd-array from nested lists of blocks.\n",
       "hstack : Stack arrays in sequence horizontally (column wise).\n",
       "dstack : Stack arrays in sequence depth wise (along third axis).\n",
       "column_stack : Stack 1-D arrays as columns into a 2-D array.\n",
       "vsplit : Split an array into multiple sub-arrays vertically (row-wise).\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> a = np.array([1, 2, 3])\n",
       ">>> b = np.array([4, 5, 6])\n",
       ">>> np.vstack((a,b))\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])\n",
       "\n",
       ">>> a = np.array([[1], [2], [3]])\n",
       ">>> b = np.array([[4], [5], [6]])\n",
       ">>> np.vstack((a,b))\n",
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Class to wrap functions with checks for __array_function__ overrides.\n",
       "\n",
       "All arguments are required, and can only be passed by position.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "dispatcher : function or None\n",
       "    The dispatcher function that returns a single sequence-like object\n",
       "    of all arguments relevant.  It must have the same signature (except\n",
       "    the default values) as the actual implementation.\n",
       "    If ``None``, this is a ``like=`` dispatcher and the\n",
       "    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n",
       "    first (additional and positional) argument.\n",
       "implementation : function\n",
       "    Function that implements the operation on NumPy arrays without\n",
       "    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n",
       "    will be forwarded to this (and the ``dispatcher``) as if using\n",
       "    ``*args, **kwargs``.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "_implementation : function\n",
       "    The original implementation passed in."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numpy.vstack?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all test labels together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mCall signature:\u001b[0m  \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m            _ArrayFunctionDispatcher\n",
       "\u001b[0;31mString form:\u001b[0m     <built-in function concatenate>\n",
       "\u001b[0;31mDocstring:\u001b[0m      \n",
       "concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=\"same_kind\")\n",
       "\n",
       "Join a sequence of arrays along an existing axis.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "a1, a2, ... : sequence of array_like\n",
       "    The arrays must have the same shape, except in the dimension\n",
       "    corresponding to `axis` (the first, by default).\n",
       "axis : int, optional\n",
       "    The axis along which the arrays will be joined.  If axis is None,\n",
       "    arrays are flattened before use.  Default is 0.\n",
       "out : ndarray, optional\n",
       "    If provided, the destination to place the result. The shape must be\n",
       "    correct, matching that of what concatenate would have returned if no\n",
       "    out argument were specified.\n",
       "dtype : str or dtype\n",
       "    If provided, the destination array will have this dtype. Cannot be\n",
       "    provided together with `out`.\n",
       "\n",
       "    .. versionadded:: 1.20.0\n",
       "\n",
       "casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
       "    Controls what kind of data casting may occur. Defaults to 'same_kind'.\n",
       "\n",
       "    .. versionadded:: 1.20.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "res : ndarray\n",
       "    The concatenated array.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "ma.concatenate : Concatenate function that preserves input masks.\n",
       "array_split : Split an array into multiple sub-arrays of equal or\n",
       "              near-equal size.\n",
       "split : Split array into a list of multiple sub-arrays of equal size.\n",
       "hsplit : Split array into multiple sub-arrays horizontally (column wise).\n",
       "vsplit : Split array into multiple sub-arrays vertically (row wise).\n",
       "dsplit : Split array into multiple sub-arrays along the 3rd axis (depth).\n",
       "stack : Stack a sequence of arrays along a new axis.\n",
       "block : Assemble arrays from blocks.\n",
       "hstack : Stack arrays in sequence horizontally (column wise).\n",
       "vstack : Stack arrays in sequence vertically (row wise).\n",
       "dstack : Stack arrays in sequence depth wise (along third dimension).\n",
       "column_stack : Stack 1-D arrays as columns into a 2-D array.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When one or more of the arrays to be concatenated is a MaskedArray,\n",
       "this function will return a MaskedArray object instead of an ndarray,\n",
       "but the input masks are *not* preserved. In cases where a MaskedArray\n",
       "is expected as input, use the ma.concatenate function from the masked\n",
       "array module instead.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> a = np.array([[1, 2], [3, 4]])\n",
       ">>> b = np.array([[5, 6]])\n",
       ">>> np.concatenate((a, b), axis=0)\n",
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])\n",
       ">>> np.concatenate((a, b.T), axis=1)\n",
       "array([[1, 2, 5],\n",
       "       [3, 4, 6]])\n",
       ">>> np.concatenate((a, b), axis=None)\n",
       "array([1, 2, 3, 4, 5, 6])\n",
       "\n",
       "This function will not preserve masking of MaskedArray inputs.\n",
       "\n",
       ">>> a = np.ma.arange(3)\n",
       ">>> a[1] = np.ma.masked\n",
       ">>> b = np.arange(2, 5)\n",
       ">>> a\n",
       "masked_array(data=[0, --, 2],\n",
       "             mask=[False,  True, False],\n",
       "       fill_value=999999)\n",
       ">>> b\n",
       "array([2, 3, 4])\n",
       ">>> np.concatenate([a, b])\n",
       "masked_array(data=[0, 1, 2, 2, 3, 4],\n",
       "             mask=False,\n",
       "       fill_value=999999)\n",
       ">>> np.ma.concatenate([a, b])\n",
       "masked_array(data=[0, --, 2, 2, 3, 4],\n",
       "             mask=[False,  True, False, False, False, False],\n",
       "       fill_value=999999)\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Class to wrap functions with checks for __array_function__ overrides.\n",
       "\n",
       "All arguments are required, and can only be passed by position.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "dispatcher : function or None\n",
       "    The dispatcher function that returns a single sequence-like object\n",
       "    of all arguments relevant.  It must have the same signature (except\n",
       "    the default values) as the actual implementation.\n",
       "    If ``None``, this is a ``like=`` dispatcher and the\n",
       "    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n",
       "    first (additional and positional) argument.\n",
       "implementation : function\n",
       "    Function that implements the operation on NumPy arrays without\n",
       "    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n",
       "    will be forwarded to this (and the ``dispatcher``) as if using\n",
       "    ``*args, **kwargs``.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "_implementation : function\n",
       "    The original implementation passed in."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numpy.concatenate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predicted labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Perform classification on samples in X.\n",
       "\n",
       "For an one-class model, +1 or -1 is returned.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
       "    For kernel=\"precomputed\", the expected shape of X is\n",
       "    (n_samples_test, n_samples_train).\n",
       "\n",
       "Returns\n",
       "-------\n",
       "y_pred : ndarray of shape (n_samples,)\n",
       "    Class labels for samples in X.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/sklearn/svm/_base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the accuracy score of the classifier on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = model.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscore\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the classifier returns one prediction for every MFCC vector in the test audio signal. Can you modify the procedure above such that the classifier returns a single prediction for a 10-second excerpt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analysis in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the MFCC features from the first test audio excerpt into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brahms = pandas.DataFrame(mfcc_brahms_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brahms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brahms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_busta = pandas.DataFrame(mfcc_busta_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the pairwise correlation of every pair of 12 MFCCs against one another for both test audio excerpts. For each audio excerpt, which pair of MFCCs are the most correlated? least correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brahms.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_busta.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a scatter plot of any two of the MFCC dimensions (i.e. columns of the data frame) against one another. Try for multiple pairs of MFCC dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brahms.plot.scatter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a scatter plot of any two of the MFCC dimensions (i.e. columns of the data frame) against one another. Try for multiple pairs of MFCC dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_busta.plot.scatter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of all values across a single MFCC, i.e. MFCC coefficient number. Repeat for a few different MFCC numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_brahms[0].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_busta[11].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new genre classifier by repeating the steps above, but this time use training data and test data from your own audio collection representing two or more different genres. For what genres and audio data styles does the classifier work well, and for which (pairs of) genres does the classifier fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new genre classifier by repeating the steps above, but this time use a different machine learning classifier, e.g. random forest, Gaussian mixture model, Naive Bayes, k-nearest neighbor, etc. Adjust the parameters. How well do they perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new genre classifier by repeating the steps above, but this time use different features. Consult the [librosa documentation on feature extraction](http://librosa.github.io/librosa/feature.html) for different choices of features. Which features work well? not well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&larr; Back to Index](index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
