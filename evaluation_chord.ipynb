{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mir_eval, librosa, numpy, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&larr; Back to Index](index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation using `mir_eval`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mir_eval` ([documentation](http://craffel.github.io/mir_eval/), [paper](http://colinraffel.com/publications/ismir2014mir_eval.pdf)) is a Python library containing evaluation functions for a variety of common audio and music processing tasks. \n",
    "\n",
    "`mir_eval` was primarily created by Colin Raffel. This notebook was created by Brian McFee and edited by Steve Tjoa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why `mir_eval`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tasks in MIR are complicated. Evaluation is also complicated!\n",
    "\n",
    "Any given task has many ways to evaluate a system. There is no one right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here are issues to consider when choosing an evaluation method:\n",
    "\n",
    "- event matching\n",
    "- time padding\n",
    "- tolerance windows\n",
    "- vocabulary alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `mir_eval` tasks and submodules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- onset, tempo, beat\n",
    "- chord, key\n",
    "- melody, multipitch\n",
    "- transcription\n",
    "- segment, hierarchy, pattern\n",
    "- separation (like `bss_eval` in Matlab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install `mir_eval`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    pip install mir_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that doesn't work:\n",
    "\n",
    "    pip install --no-deps mir_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Onset Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('audio/simple_piano.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate onsets.\n",
    "est_onsets = librosa.onset.onset_detect(y=y, sr=sr, units='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27863946, 0.510839  , 0.81269841, 1.021678  , 1.32353741,\n",
       "       1.50929705, 1.83437642, 2.02013605, 2.36843537, 2.53097506,\n",
       "       2.87927438, 3.0185941 , 3.36689342, 3.59909297])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference annotation.\n",
    "ref_onsets = numpy.array([0.1, 0.21, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('F-measure', 0.11764705882352941),\n",
       "             ('Precision', 0.07142857142857142),\n",
       "             ('Recall', 0.3333333333333333)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mir_eval.onset.evaluate(ref_onsets, est_onsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mir_eval` finds the largest feasible set of matches using the [Hopcroft-Karp algorithm](https://en.wikipedia.org/wiki/Hopcroft%E2%80%93Karp_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Beat Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_tempo, est_beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "est_beats = librosa.frames_to_time(est_beats, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53405896, 1.021678  , 1.53251701, 2.04335601, 2.53097506])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the reference annotation.\n",
    "ref_beats = numpy.array([0.53, 1.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gijsm/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/mir_eval/beat.py:91: UserWarning: Reference beats are empty.\n",
      "  warnings.warn(\"Reference beats are empty.\")\n",
      "/Users/gijsm/Github/iranroman/musicinformationretrieval.com/venv/lib/python3.11/site-packages/mir_eval/beat.py:93: UserWarning: Estimated beats are empty.\n",
      "  warnings.warn(\"Estimated beats are empty.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('F-measure', 0.0),\n",
       "             ('Cemgil', 0.0),\n",
       "             ('Cemgil Best Metric Level', 0.0),\n",
       "             ('Goto', 0.0),\n",
       "             ('P-score', 0.0),\n",
       "             ('Correct Metric Level Continuous', 0.0),\n",
       "             ('Correct Metric Level Total', 0.0),\n",
       "             ('Any Metric Level Continuous', 0.0),\n",
       "             ('Any Metric Level Total', 0.0),\n",
       "             ('Information gain', 0.0)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mir_eval.beat.evaluate(ref_beats, est_beats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Chord Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mir_eval.chord.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden benefits\n",
    "\n",
    "- Input validation! Many errors can be traced back to ill-formatted data.\n",
    "- Standardized behavior, full test coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mir_eval has tools for display and sonification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import mir_eval.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common plots: `events`, `labeled_intervals`\n",
    "\n",
    "pitch, multipitch, piano_roll\n",
    "segments, hierarchy,\n",
    "separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m librosa\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mspecshow(\u001b[43mS\u001b[49m, x_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, y_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m mir_eval\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mevents(ref_beats, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      3\u001b[0m mir_eval\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mevents(est_beats, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'S' is not defined"
     ]
    }
   ],
   "source": [
    "librosa.display.specshow(S, x_axis='time', y_axis='mel')\n",
    "mir_eval.display.events(ref_beats, color='w', alpha=0.8, linewidth=3)\n",
    "mir_eval.display.events(est_beats, color='c', alpha=0.8, linewidth=3, linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Labeled Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Source Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_harm, y_perc = librosa.effects.hpss(y, margin=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "mir_eval.display.separation([y_perc, y_harm], sr, labels=['percussive', 'harmonic'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=numpy.vstack(["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mir_eval.sonify.chords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&larr; Back to Index](index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
